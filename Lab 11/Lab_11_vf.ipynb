{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**<h1><center>Laboratorio 11: LLM y Agentes Autónomos 🤖</center></h1>**\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos</strong></center>"
      ],
      "metadata": {
        "id": "PyPTffTLug7i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD8X1uhGzAHq",
        "cell_id": "737a4540885f41acb34b9863a968b907",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesor: Ignacio Meza, Sebastian Tinoco\n",
        "- Auxiliar: Catherine Benavides, Consuelo Rojas\n",
        "- Ayudante: Eduardo Moya, Nicolás Ojeda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXflExjqzAHr",
        "cell_id": "e4a6f26138654eb49ee963fb4c7ecf46",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### Equipo: **SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
        "\n",
        "- Nombre de alumno 1: Sergio Rehbein\n",
        "- Nombre de alumno 2: Matías Cornejo\n",
        "\n",
        "**SUPER IMPORTANTE** - notebooks sin nombre no serán revisados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD-V0bbZzAHr",
        "owner_user_id": "badcc427-fd3d-4615-9296-faa43ec69cfb",
        "cell_id": "7dd4aaebd4f44063aedbb47ea36349a5",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "\\### **Link de repositorio de GitHub Matias:** https://github.com/s-kill/MDS7202\n",
        "\n",
        "\\### **Link de repositorio de GitHub Sergio:** https://github.com/sergiorehbein/MDS7201---Proyecto-de-Ciencia-de-Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcnsiQMkzAHr",
        "cell_id": "abe08e51696a471e8cc8ac1fa4216f0b",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### **Indice**\n",
        "\n",
        "1. [Temas a tratar](#Temas-a-tratar:)\n",
        "3. [Descripcción del laboratorio](#Descripción-del-laboratorio.)\n",
        "4. [Desarrollo](#Desarrollo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uBLPj1PzAHs",
        "cell_id": "0174e9377ebb43eaa0d12718db4c81ec",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## **Temas a tratar**\n",
        "\n",
        "- Implementación de modelos de LLM y Reinforcement Learning.\n",
        "- Utilización e implementación de agentes.\n",
        "\n",
        "## **Reglas:**\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 7 días desde la publicación, 3 días de atraso con 1 punto de descuento c/u.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria.\n",
        "- Prohibidas las copias. Cualquier intento de copia será debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no estén en u-cursos no serán revisados. Recuerden que el repositorio también tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "Pueden usar cualquer material del curso que estimen conveniente.\n",
        "\n",
        "### **Objetivos principales del laboratorio**\n",
        "\n",
        "- Generar un modelo LLM generativo interactivo.\n",
        "- Entrenar un modelo de Reinforce Learning.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Large Language Models (4.0 puntos)**"
      ],
      "metadata": {
        "id": "Is4P4NDMurx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://trestristescriticos.com/wp-content/uploads/2021/07/telefono-gratuito-cinesur.jpg\" width=\"350\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "tJ3yV96HwN75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Joaquín no es un aficionado del cine, pero a principios de año, se propuso ver más peliculas para poder tener más temas de conversación con sus amigos y familia. Sin embargo, ya es junio y Joaquín no ha visto ninguna pelicula nueva o relevante de las que tenía en su lista y su reunión familiar bi-anual se acerca y necesita la mayor información que pueda recopilar de dichas peliculas sin tener que verlas.\n",
        "\n",
        "Para esto, usted con su compañerx, tendrá que crear una aplicación utilizando LangChain.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kB8z1qrGww4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instalación de librerías**\n",
        "\n",
        "Para la creación de la aplicación, se utilizara un modelo de lenguaje (LLM) ofrecido gratuitamente por Google.\n",
        "\n",
        "Para ello, se utilizará la API de Gemini, por lo que si no tienen acceso, se pueden crear una cuenta en el siguiente [enlace a Google AI](https://ai.google.dev/). Ahí, ir a la pestaña superior y seleccione la opción que dice ``Gemini API``.\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-06-13_at_12.42.32_PM.png' width='450' />\n",
        "\n",
        "Luego, seleccione el botón que dice ``Get API key in Google AI Studio`` y hacer click en ``Crear clave de API`` para generar la llave con la que se podrá consultar al modelo de lenguaje.\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-06-13_at_12.45.10_PM.png?ref_type=heads' width='450' />\n",
        "\n",
        "**Importante:** Debido a las restricciones de esta API, lo ideal es utilizar la llave a la API de manera personal.\n",
        "\n",
        "\n",
        "Para mayor información sobre **LangChain**, pueden revisar la documentación en el [presente enlace](https://python.langchain.com/v0.2/docs/tutorials/summarization/ )."
      ],
      "metadata": {
        "id": "dOIeEP9Ey_lF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CS_6MjRoWYMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install langchain_google_genai\n",
        "!pip install langchain-community\n",
        "!pip install langchain-experimental\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "LLbYWURudw2c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "82aJnnH0b0Oo"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDB95Ktc4MiqMcz8Z6xFx108_wpylgQTqo\" #AIzaSyDB95Ktc4MiqMcz8Z6xFx108_wpylgQTqo\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Carga y limpieza (0.5 puntos)**"
      ],
      "metadata": {
        "id": "kUgbzVtWUYq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para iniciar su titanica tarea de enseñarle a Joaquín sobre las mejores peliculas del último tiempo, tiene que revisar los script de las siguientes 3 peliculas:\n",
        "* Dune 2\n",
        "* Under Paris\n",
        "* Joker\n",
        "\n",
        "Debe encontrar un patrón y obtener solamente el guión de las películas. Para ello se recomienda utilizar métodos de búsqueda y reemplazo que tienen los ``string`` en Python. Adicionalmente, puede usar filtros de expresiones regulares.\n",
        "\n",
        "Posterior a la limpieza de los guiones, debe considerar que el patrón se repite y es generalizable.\n"
      ],
      "metadata": {
        "id": "6I10Li9a7nez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scripts de peliculas\n",
        "dune2_script=\"https://scrapsfromtheloft.com/movies/dune-part-two-2024-transcript/\"\n",
        "underparis_script=\"https://scrapsfromtheloft.com/movies/under-paris-2024-transcript/\"\n",
        "joker_script=\"https://scrapsfromtheloft.com/movies/joker-2019-transcript/\""
      ],
      "metadata": {
        "id": "HpYuwfO_F0pD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "\n",
        "def load_website_data(url):\n",
        "\n",
        "  loader = WebBaseLoader(url)\n",
        "  website_data = loader.load()\n",
        "\n",
        "  return website_data[0].page_content\n",
        "\n",
        "def remove_text_before_marker(text):\n",
        "    match = re.search(\"\\t\\t\\n\\n\\n\\n \\n\\n\\n\\n\", text)\n",
        "    body = text[match.end():]\n",
        "\n",
        "    if \"* * *\" in body:\n",
        "        match = re.search(\"\\* \\* \\*\", body)\n",
        "\n",
        "        text_out = body[match.end():]\n",
        "    else:\n",
        "        text_out = body\n",
        "\n",
        "    match = re.search('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', text_out)\n",
        "    text_out = text_out[:match.start()]\n",
        "    return text_out"
      ],
      "metadata": {
        "id": "XUfvpxPD8v5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49c7a5c-6f61-4a69-ddb0-c6d38799867c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dune2 = load_website_data(dune2_script)\n",
        "underparis = load_website_data(underparis_script)\n",
        "joker = load_website_data(joker_script)"
      ],
      "metadata": {
        "id": "wXKJI15ky1Os"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dune2_text = remove_text_before_marker(dune2)\n",
        "underparis_text = remove_text_before_marker(underparis)\n",
        "joker_text = remove_text_before_marker(joker)"
      ],
      "metadata": {
        "id": "HeNVKEldy5fV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Aplicación (3.5 puntos)**\n",
        "\n",
        "Luego de limpiar los guiones, es posible generar la aplicaicón deseada con el LLM. Esta aplicación tiene que ser capaz de realizar las siguientes tareas.\n",
        "\n",
        "1. Utilizando una plantilla sobre el nombre del archivo o la URL, identifique el supuesto nombre de la película.\n",
        "\n",
        "2. Genere un resumen en español de la película y una nota evaluativa sobre la misma. El resumen debe tener entre 3 a 5 párrafos. Además, obtener una evaluación de la película con una calificación del 1 al 10, utilizando una LLM y el contexto entregado"
      ],
      "metadata": {
        "id": "7Btad-nZ9EyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.1 Título de la película (0.5 puntos)**\n",
        "\n",
        "Para obtener el título, utilicé la siguiente plantilla:\n",
        "```\n",
        " template = \"\"\"\n",
        "  What is the movie that appears in the description of this file or url?\n",
        "  You only give me the movie name, nothing more.\n",
        "  document/url: {script_path_url}\n",
        "  \"\"\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QcS80oN2-Gq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_title(script_path_url):\n",
        "  template = \"\"\"\n",
        "  What is the movie that appears in the description of this file or url?\n",
        "  You only give me the movie name, nothing more.\n",
        "  document/url: {script_path_url}\n",
        "  \"\"\"\n",
        "  prompt = PromptTemplate.from_template(template)\n",
        "  prompt_val = prompt.invoke({\"script_path_url\": script_path_url})\n",
        "  result = llm.invoke(prompt_val)\n",
        "\n",
        "  return result.content.strip()"
      ],
      "metadata": {
        "id": "yNIU3mmh-F5W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.2 Resumen (1.0 puntos)**\n",
        "\n",
        "Como se vió en clases, las LLM no pueden manejar cadenas de texto muy largas, esto es debido a que, dependiendo de su naturaleza, solo manejan ventanas de contexto que estan asociadas a caracteristicas de la red y del entrenamiento utilizado.\n",
        "\n",
        "Por ello, es altamente importante que si se desea hacer un resumen del texto, este se haga realizando un tipo de map/reduce sobre el texto. De manera que en cada una de las iteraciones se vaya disminuyendo el tamaño del texto, pero hay que tener cuidado con que le modelo vaya guardando el contexto de escenas previas."
      ],
      "metadata": {
        "id": "muDXLfr0CabX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.chains import StuffDocumentsChain, LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "9sxX87HpDZiV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#No cambiar función\n",
        "\n",
        "def map_reduce_text(script, map_template, reduce_template):\n",
        "\n",
        "  # Map\n",
        "  \"\"\"\n",
        "  map_prompt, crear el prompt desde el template\n",
        "  map_chain, crear la cadena desde el prompt\n",
        "  \"\"\"\n",
        "  map_prompt = PromptTemplate.from_template(map_template)\n",
        "  map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "  # Reduce\n",
        "  \"\"\"\n",
        "  reduce_prompt, crear el prompt desde el template\n",
        "  reduce_chain, crear la cadena desde el prompt\n",
        "  \"\"\"\n",
        "  reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "  reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "  # Combine\n",
        "  \"\"\"\n",
        "  Combinar y reducir los documentos, utilizar StuffDocumentsChain\n",
        "  y ReduceDocuentsChain con un máximo de 4000 tokens\n",
        "  \"\"\"\n",
        "  combine_documents_chain = StuffDocumentsChain(\n",
        "      llm_chain=reduce_chain,\n",
        "      document_variable_name=\"document\",\n",
        "  )\n",
        "\n",
        "\n",
        "  reduce_documents_chain = ReduceDocumentsChain(\n",
        "      combine_documents_chain = combine_documents_chain,\n",
        "      collapse_documents_chain = combine_documents_chain,\n",
        "      token_max = 4000,\n",
        "  )\n",
        "\n",
        "  # Map/Reduce\n",
        "  \"\"\"\n",
        "  Uilizar MapReduceDocumentsChain\n",
        "  \"\"\"\n",
        "\n",
        "  map_reduce_chain = MapReduceDocumentsChain(\n",
        "    llm_chain=map_chain,\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    document_variable_name=\"document\",\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "  # Text splitter\n",
        "  \"\"\"\n",
        "  Usar RecursiveCharacterTextSplitter\n",
        "  \"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 15000,\n",
        "      chunk_overlap = 0\n",
        "  )\n",
        "\n",
        "  split_script = text_splitter.split_text(script)\n",
        "\n",
        "\n",
        "  # resultado\n",
        "  split_script = text_splitter.create_documents(split_script)\n",
        "\n",
        "  #result = map_reduce_chain.run()\n",
        "  #return result\n",
        "\n",
        "  result = map_reduce_chain.invoke(split_script)\n",
        "\n",
        "  return result[\"output_text\"]"
      ],
      "metadata": {
        "id": "jkxFMnCFDcgl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crear templates\n",
        "\n",
        "map_template_summary = \"\"\"\n",
        "Resume la siguiente parte del guion. Mantén los detalles importantes y el contexto:\n",
        "{document}\n",
        "\"\"\"\n",
        "\n",
        "reduce_template_summary = \"\"\"\n",
        "Combina los siguientes resúmenes en un solo resumen cohesivo. Asegúrate de mantener los detalles importantes y el contexto:\n",
        "{document}\n",
        "\"\"\"\n",
        "\n",
        "answer_summary = \"\"\"\n",
        "Genere un resumen en español del documento y una nota evaluativa sobre el mismo. El resumen debe tener entre 3 a 5 párrafos. Además, obtener una evaluación de la película con una calificación del 1 al 10.\n",
        "Este resumen debe ser creado SOLO a partir de esta versión resumida del guion proporcionada a continuación. No debe incluir información externa o conocimiento previo sobre la película. Aquí está el guion resumido:\n",
        "{document}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HsEJR0IGEZ8V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir resumenes de películas.\n",
        "dune2_reduce_summary = map_reduce_text(dune2_text, map_template_summary, reduce_template_summary)\n",
        "underparis_reduce_summary = map_reduce_text(underparis_text, map_template_summary, reduce_template_summary)\n",
        "joker_reduce_summary = map_reduce_text(joker_text, map_template_summary, reduce_template_summary)\n",
        "\n",
        "prompt = PromptTemplate.from_template(answer_summary)\n",
        "dune2_resumen = llm.invoke(prompt.invoke({\"document\": dune2_reduce_summary}))\n",
        "print(dune2_resumen.content)\n",
        "\n",
        "underparis_resumen = llm.invoke(prompt.invoke({\"document\": underparis_reduce_summary}))\n",
        "print(underparis_resumen.content)\n",
        "\n",
        "joker_resumen = llm.invoke(prompt.invoke({\"document\": joker_reduce_summary}))\n",
        "print(joker_resumen.content)"
      ],
      "metadata": {
        "id": "ijib42GaIFSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c63d8c-813c-4711-a8a0-3eba588cd7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Resumen de Dune\n",
            "\n",
            "\"Dune\" nos introduce en un universo de intrigas políticas y luchas por el poder, centrado en el planeta desértico de Arrakis, hogar de la especia, una sustancia de gran valor que otorga longevidad y habilidades psíquicas. La historia sigue a Paul Atreides, un joven noble que se convierte en el salvador profetizado de los Fremen, un pueblo indígena de Arrakis. \n",
            "\n",
            "Tras la destrucción de la Casa Atreides a manos de los Harkonnen, sus enemigos, Paul y su madre, Jessica, se refugian entre los Fremen. Aunque inicialmente recelosos, los Fremen reconocen a Paul como el Lisan al-Gaib, un salvador profetizado, y a Jessica como la nueva Madre Superiora. Paul se integra a la cultura Fremen, aprendiendo sus costumbres y habilidades, ganándose el respeto de su líder, Stilgar. Se convierte en un Fedaykin, un guerrero Fremen, y recibe el nombre de Muad'Dib, \"El que señala el camino\".\n",
            "\n",
            "Mientras tanto, los Harkonnen, liderados por el Barón Harkonnen y sus hijos, buscan destruir a los Fremen y controlar la producción de especia. Paul, con la ayuda de los Fremen, se enfrenta a los Harkonnen, emergiendo como un líder carismático y poderoso. \n",
            "\n",
            "La historia se intensifica con el descubrimiento del arsenal atómico de los Atreides, que Paul podría usar para vengarse de los Harkonnen. Sin embargo, él busca una solución pacífica, aunque la presión de los Fremen y los ataques continuos de los Harkonnen lo obligan a tomar las riendas de la lucha por la libertad. \n",
            "\n",
            "Finalmente, Paul y los Fremen se enfrentan a los Sardaukar, los soldados de élite del Emperador. En una batalla épica, Paul vence al Barón Harkonnen y se revela como el Lisan al-Gaib. Su victoria amenaza el control del Emperador sobre Arrakis, provocando la preparación de las Grandes Casas para invadir el planeta. \n",
            "\n",
            "Paul, con su nuevo poder y la lealtad de los Fremen, amenaza con destruir los campos de especia si las Grandes Casas atacan, desencadenando una guerra santa que cambiará el destino de Arrakis y el universo conocido. La historia termina con Paul, ahora conocido como Muad'Dib, como el líder de los Fremen, listo para enfrentarse a las Grandes Casas y luchar por la libertad de su pueblo, dejando al público en suspenso ante el desenlace de esta épica batalla por el control de Arrakis.\n",
            "\n",
            "\n",
            "## Evaluación\n",
            "\n",
            "La historia de \"Dune\", a pesar de su complejidad, se presenta de forma clara y atractiva. La trama, rica en intrigas políticas, conflictos morales y acción, mantiene al lector interesado hasta el final. La construcción de los personajes, especialmente Paul, es convincente, mostrando su evolución desde un joven noble a un líder carismático y poderoso. \n",
            "\n",
            "La exploración de temas como la profecía, la libertad, la responsabilidad y la lucha por el poder, junto a la descripción detallada del mundo de Arrakis, enriquece la experiencia del lector.  Sin embargo, la historia se queda en un punto álgido, dejando al lector con la incertidumbre del desenlace de la batalla por Arrakis. \n",
            "\n",
            "**Calificación: 8/10** \n",
            "\n",
            "## Resumen de \"Lilith\"\n",
            "\n",
            "La historia comienza con un equipo de científicos marinos que estudian el \"séptimo continente\", una enorme masa de basura en el Océano Pacífico Norte. Su investigación se centra en Lilith, un tiburón Mako anormalmente grande y agresivo. Durante su estudio, descubren un grupo de tiburones Mako hembras de tamaño inusual que cazan en manada. Chris, uno de los buzos, desaparece tras un encuentro con Lilith.\n",
            "\n",
            "Tres años después, en París, se descubre un proyectil de la Segunda Guerra Mundial en el Sena. Sophia Assalas, de la organización SOS, sospecha que algo no está bien y descubre que Lilith está en el río. Sophia cree que Lilith necesita ser rescatada, y convence al sargento Adil Faez de que la acompañe a buscarla.\n",
            "\n",
            "Juntos encuentran a Lilith cerca de la Isla de la Ciudad. El equipo se prepara para rescatarla, pero Sophia les advierte del peligro que representa. Mientras tanto, Mika y Ben intentan atraer a Lilith de vuelta al océano usando una baliza reactivada. \n",
            "\n",
            "Adil y su equipo descubren que la señal de la baliza ha sido interferida, y Sophia sospecha de Mika. Mika y su grupo se adentran en las catacumbas, donde se encuentran con un grupo de personas. \n",
            "\n",
            "Adil y su equipo llegan a las catacumbas y se encuentran con Mika y su grupo, pero Lilith aparece y se desata el caos. El equipo intenta evacuar las catacumbas, con muchas personas atrapadas y en peligro. \n",
            "\n",
            "La escena cambia a un hospital, donde Sophia está en shock tras la muerte de Leo. Caro y Markus informan a Adil sobre el descubrimiento de un nuevo tipo de tiburón en las catacumbas de París, llamado Lilith. Lilith es capaz de reproducirse por partenogénesis y se ha adaptado al agua dulce, lo que representa una amenaza para la ciudad.\n",
            "\n",
            "El prefecto ordena ocultar la información, y Adil, Sophia y Adama planean infiltrarse en las catacumbas para destruir el nido de Lilith. Durante la operación, Poiccard es atacado y muere. El equipo logra escapar y se dirige a la superficie.\n",
            "\n",
            "Mientras tanto, la competición de triatlón comienza, y Lilith ataca a los nadadores. Los soldados del ejército disparan a los tiburones, provocando una explosión masiva.\n",
            "\n",
            "Adil, Sophia y Caro logran escapar de las catacumbas, y Sophia se queda atrás para ayudar a Adil, que está herido. La escena termina con Sophia y Adil en el agua, rodeados de tiburones, dejando al espectador con la sensación de que la amenaza de los tiburones aún no ha terminado. \n",
            "\n",
            "## Nota Evaluativa\n",
            "\n",
            "El guion de \"Lilith\" presenta una historia llena de acción y suspense, con una trama que combina elementos de ciencia ficción, horror y thriller. La premisa de un tiburón gigante y adaptado al agua dulce que amenaza la ciudad de París es intrigante y crea una atmósfera de tensión constante.\n",
            "\n",
            "La historia tiene un ritmo dinámico y mantiene al espectador en vilo con giros inesperados y momentos de alta intensidad. La relación entre Sophia y Adil, dos personajes con personalidades y motivaciones diferentes, aporta un toque de complejidad emocional a la trama.\n",
            "\n",
            "Sin embargo, el guion podría beneficiarse de un desarrollo más profundo de algunos personajes y de una exploración más exhaustiva de las consecuencias sociales y políticas que conlleva la amenaza de Lilith. \n",
            "\n",
            "**Calificación: 7/10** \n",
            "\n",
            "## Resumen:\n",
            "\n",
            "El guion nos presenta a Arthur Fleck, un hombre con problemas mentales que lucha por sobrevivir en la caótica Gotham City. La huelga de basureros ha sumido la ciudad en el caos, reflejando la propia lucha interna de Arthur. Recién salido de un hospital psiquiátrico, Arthur busca un aumento en su medicación, pero su soledad y aislamiento lo hacen vulnerable a las presiones de la vida. \n",
            "\n",
            "Arthur se aferra a la esperanza de encontrar un espacio en la sociedad, anhelando la atención y la admiración de los demás. Su obsesión con el comediante Murray Franklin y su fijación con Thomas Wayne, el jefe de su madre, Penny, reflejan su deseo de ser reconocido y su búsqueda de una figura paternal.\n",
            "\n",
            "Sin embargo, su deseo de atención se convierte en un comportamiento errático y peligroso. La pérdida de su trabajo, tras ser descubierto llevando un arma al hospital, lo lleva a un punto de quiebre. El guion termina con Arthur, sin trabajo y desesperado por ser visto, dando un paso hacia la oscuridad y la violencia. \n",
            "\n",
            "## Evaluación:\n",
            "\n",
            "El guion presenta una premisa interesante, explorando la fragilidad mental de un individuo en un entorno social caótico. La historia de Arthur, un hombre marginado que busca reconocimiento, es convincente y genera empatía. Sin embargo, el guion carece de detalles sobre la evolución de Arthur hacia la violencia, dejando al lector con preguntas sobre los motivadores detrás de su comportamiento. \n",
            "\n",
            "La falta de profundidad en el desarrollo de los personajes secundarios, como Penny y Randall, debilita la construcción del mundo y la conexión emocional con Arthur. La historia se centra en la lucha interna de Arthur, pero la falta de contexto sobre su pasado y sus relaciones limita la comprensión de sus acciones.\n",
            "\n",
            "**Calificación: 6/10**\n",
            "\n",
            "El guion presenta un potencial interesante, pero necesita un desarrollo más profundo de los personajes y la trama para alcanzar su máximo potencial. La falta de contexto y la falta de claridad en la evolución de Arthur hacia la violencia debilita la historia. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adicionalmente, Joaquín sabe que su primo favorito le gusta ``Dune: Part 2`` por lo que le gustaría tener mayor información al respecto, para ello realice las siguientes tareas:\n",
        "\n",
        "\n",
        "3. Genere un gráfico que muestre los personajes de la película con más apariciones en la misma.\n",
        "4. Genere una tabla en pandas con los 3 personajes que más aparecen, indicando el nombre del actor y su edad actual más uno (ojo edad + 1).\n",
        "5. Cree una función que responda preguntas sobre la película basándose en la información del texto entregado (OJO: las preguntas y salidas deben ser en español). Luego, responda las siguientes preguntas:\n",
        "* ¿Qué y quién es Lisan al-Gaib?\n",
        "* ¿Qué personaje no cree en la profecía pero es parte de ella?\n",
        "* ¿Cuál es el objetivo de Feyd-Rautha?\n",
        "6. Utilizando el top 3 de personajes que más aparecen en la película, genere con el modelo LLM y utilizando el contexto del guion, las 6 estadísticas que demuestren las habilidades de los personajes: Intelligence, Strength, Charisma, Wisdom, Emotional Resilience, y Creativity."
      ],
      "metadata": {
        "id": "dD7YHYZSIWbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.3 Personajes (0.5 puntos)**\n",
        "\n",
        "En la siguiente sección, tiene que entregar un template de personajes y redicción"
      ],
      "metadata": {
        "id": "e_vdMMceJZBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_template_characters = \"\"\"\n",
        "Identifica los personajes en la siguiente parte del guion y cuenta sus apariciones. Incluye el nombre del personaje y el número de apariciones en este segmento:\n",
        "{document}\n",
        "\"\"\"\n",
        "\n",
        "reduce_template_characters = \"\"\"\n",
        "Combina las siguientes listas de personajes y sus apariciones en una sola lista cohesiva. Suma las apariciones de cada personaje:\n",
        "{document}\n",
        "\n",
        "El resultado debe tener el siguiente formato JSON:\n",
        "[\n",
        "  {{\"personaje\": \"Nombre del personaje\", \"apariciones\": Número de apariciones}},\n",
        "  ...\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "answer_character_list = map_reduce_text(\n",
        "    dune2_text,\n",
        "    map_template_characters,\n",
        "    reduce_template_characters\n",
        ")"
      ],
      "metadata": {
        "id": "fIM5JVC5JWNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from itertools import count\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "\n",
        "def plot_characters(answer_character_list):\n",
        "  # Clear answer\n",
        "  answer_character_list = re.sub(r'\\s+', ' ', answer_character_list).strip()[8:-3]\n",
        "  character_pd = pd.DataFrame(json.loads(answer_character_list))\n",
        "\n",
        "  # distil the characters output\n",
        "  \"\"\"\n",
        "  Recomendación, utilizar un diccionario para ordenar los personajes\n",
        "  \"\"\"\n",
        "  # Ordenar personajes por aparición\n",
        "  character_pd = character_pd.sort_values(by='apariciones', ascending=False)\n",
        "\n",
        "\n",
        "  # Create dataframe\n",
        "  \"\"\"\n",
        "  De diccionario a DataFrame\n",
        "  \"\"\"\n",
        "  Top3_characters = character_pd.head(3)\n",
        "\n",
        "  # Graficar datos\n",
        "  fig = px.bar(character_pd, x='personaje', y='apariciones', title='Apariciones de Personajes en la Película')\n",
        "  fig.show()\n",
        "\n",
        "  #Retornar los personajes\n",
        "  return character_pd['personaje']\n"
      ],
      "metadata": {
        "id": "hJQ-RPYJKOKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista_personajes = plot_characters(answer_character_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Xx81ShvtyPhs",
        "outputId": "1658c9b7-b890-4327-ccf7-2b9ecdc650f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"2845352b-a0c7-4b0e-9c35-67cb0446cd5a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2845352b-a0c7-4b0e-9c35-67cb0446cd5a\")) {                    Plotly.newPlot(                        \"2845352b-a0c7-4b0e-9c35-67cb0446cd5a\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"personaje=%{x}\\u003cbr\\u003eapariciones=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"Stilgar\",\"Paul\",\"Harkonnen Soldiers (general)\",\"Jessica\",\"Chani\",\"Fremen (general)\",\"Gurney Halleck\",\"Fremen\",\"Harkonnen soldier\",\"Fedaykin fighter\",\"Rabban\",\"Feyd-Rautha Harkonnen\",\"Shishakli\",\"Man 1\",\"Baron Harkonnen\",\"Reverend Mother Mohiam\",\"Man (no identificado)\",\"Lady Margot Fenring\",\"Irulan\",\"Harkonnen Commander\",\"Gladiator arena announcer\",\"Sardaukar\",\"Bashar\",\"Alia\",\"Oldest Elder\",\"Usul\",\"Man 2\",\"Harkonnen Squad Leader\",\"Harkonnen Soldier 1\",\"Emperor\",\"Bene Gesserit sister 1\",\"Lanville\",\"Fremen Nuns\",\"Ancient Voice 2\",\"Ancient Voice 1\",\"Bene Gesserit sister 2\",\"Maker Keeper\",\"Jamis\",\"Watermasters\",\"Harkonnen Sniper\",\"Slave master\",\"Male Watermaster\",\"Sentinel Leader\",\"Old Watermaster\",\"Fremen Sentinel\",\"Harkonnen Lieutenant\",\"Guard\",\"Woman\",\"Man on Radio\",\"Man\",\"Commander\",\"Translator\",\"Elders\"],\"xaxis\":\"x\",\"y\":[48,44,34,33,31,26,23,16,15,12,11,10,10,7,7,6,6,6,5,5,4,4,4,4,4,4,4,3,3,3,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"personaje\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"apariciones\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Apariciones de Personajes en la Película\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2845352b-a0c7-4b0e-9c35-67cb0446cd5a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.4 Actores principales (0.75 puntos)**\n",
        "\n",
        "Importante saber que el script **no** maneja información de los actores, por ello, es importante que nuestra LLM tenga acceso a internet, de manera de poder realizar búsquedas que nos ayuden a completar la información consultada.\n",
        "\n",
        "Para esto, utilizaremos agentes combinados con react para realzar la consulta y asegurarnos de que la respuesta es correcta."
      ],
      "metadata": {
        "id": "2G2dx0XoLis4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import AgentType, initialize_agent"
      ],
      "metadata": {
        "id": "A9CHzsLDKPeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key para realizar una busqueda\n",
        "os.environ[\"SERPER_API_KEY\"] = 'd63e62662ef63eb9e44ab133d191f7a99a0024a3'"
      ],
      "metadata": {
        "id": "4ZSclMoFMGSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWQpu32y-UC8",
        "outputId": "934d3258-4ea2-49d8-a65f-902cc3aeb55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning:\n",
            "\n",
            "The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lista_personajes.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyQ_m8c9-cOg",
        "outputId": "03bfab4c-17c9-4932-b01d-8b96bb38fc2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3                         Stilgar\n",
              "1                            Paul\n",
              "6    Harkonnen Soldiers (general)\n",
              "Name: personaje, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "def get_actors_and_age(character):\n",
        "\n",
        "  # Inicializar tools y agente.\n",
        "  tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
        "  agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "  # Crear template de query\n",
        "  query_template_nombre = \"\"\"\n",
        "  Who plays {character} in Dune 2?. Answer must be only the actor name.\n",
        "  \"\"\"\n",
        "\n",
        "  query_template_edad = \"\"\"\n",
        "  \"What is the actor {actor_name} birth year?\"\n",
        "  \"\"\"\n",
        "\n",
        "  # Crear prompt y usar agente para la búsqueda.\n",
        "  prompt_nombre = PromptTemplate.from_template(query_template_nombre)\n",
        "  prompt_edad = PromptTemplate.from_template(query_template_edad)\n",
        "\n",
        "  # Retornar Nombre y Edad + 1\n",
        "  nombre_actor = agent.invoke(prompt_nombre.invoke({\"character\": character}))['output']\n",
        "  edad_actor = agent.invoke(prompt_edad.invoke({\"actor_name\": nombre_actor}))['output']\n",
        "\n",
        "  try:\n",
        "    birth_year = int(edad_actor.strip())\n",
        "    current_year = datetime.datetime.now().year\n",
        "    actor_age_p1 = current_year - birth_year + 1\n",
        "  except ValueError:\n",
        "    actor_age_p1 = \"Edad no encontrada\"\n",
        "\n",
        "  return nombre_actor, actor_age_p1"
      ],
      "metadata": {
        "id": "sbGuaD6PMMA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nombre, edad = get_actors_and_age('Stilgar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKneceinAznV",
        "outputId": "a03932a9-8be5-4ec7-d0d0-4c4f0da9e3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find out who plays Stilgar in Dune 2. \n",
            "Action: google_serper\n",
            "Action Input: Who plays Stilgar in Dune 2?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mJavier Bardem\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the answer.\n",
            "Final Answer: Javier Bardem \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find the birth year of Javier Bardem. \n",
            "Action: google_serper\n",
            "Action Input: Javier Bardem birth year\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m1969\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: 1969 \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nombre, edad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU9P8ql9H9Ma",
        "outputId": "a7bf2772-16d9-4c62-acca-d1d1d99c5d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Javier Bardem 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tras distintas iteraciones de Prompts, descubrimos que preguntas con respuestas simples evitaban que google serper entrara en loop, por lo tanto decidimos separar la pregunta del nombre y la edad del actor por separado. También descubrimos que dado que es más sencillo obtener la fecha de nacimiento que la edad calculada, preguntamos por el año de nacimiento e hicimos la resta (más uno) para obtener la edad deseada."
      ],
      "metadata": {
        "id": "Kpf9H61qMxal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.5 Personajes Stats (0.5 puntos)**\n",
        "\n",
        "Esta parte es similar al punto 2. La clave esta en crear un buen prompting que nos permita generar las estadísticas basandonos en una búsqueda por map/reduce.\n",
        "\n",
        "Tras la búsqueda, la idea es tener una función de Python que nos permita generar el gráfico deseado y tener el resumen de los personajes.\n"
      ],
      "metadata": {
        "id": "MtQqA40sM09E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def map_reduce_text(script, character):\n",
        "  # Map\n",
        "  map_template = \"\"\"\n",
        "  You are analyzing a movie script. Extract and analyze the following attributes for the character {character}:\n",
        "  Intelligence, Charisma, Strength, Wisdom, Emotional Resilience, and Creativity.\n",
        "  Provide your analysis in a clear and detailed manner.\n",
        "  \"\"\"\n",
        "\n",
        "  # crear prompt y cadena\n",
        "  #map_template += template_complemt\n",
        "  map_prompt = PromptTemplate.from_template(map_template)\n",
        "  map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "  # Reduce\n",
        "  reduce_template = \"\"\"\n",
        "  Based on the provided analysis, rate the character {character} on a scale from 1 to 10 for the following attributes:\n",
        "  Intelligence, Charisma, Strength, Wisdom, Emotional Resilience, and Creativity.\n",
        "  Provide a summary for each attribute explaining the rating in spanish.\n",
        "  Besides the summary add a json format text:\n",
        "  [\n",
        "    {{\"Intelligence\": Intelligence Rate, \"Charisma\": Charisma rate, ...}},\n",
        "    ...\n",
        "  ]\n",
        "\n",
        "  Split the summary from the json text with the text ***\n",
        "  \"\"\"\n",
        "  reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "  reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "  # Reduce\n",
        "  \"\"\"\n",
        "  Reducir y combinar los documentos con un máximo de 4000 tokens\n",
        "  \"\"\"\n",
        "  combine_documents_chain = StuffDocumentsChain(\n",
        "      llm_chain=reduce_chain,\n",
        "      document_variable_name=\"character\",\n",
        "  )\n",
        "\n",
        "  reduce_documents_chain = ReduceDocumentsChain(\n",
        "      combine_documents_chain = combine_documents_chain,\n",
        "      collapse_documents_chain = combine_documents_chain,\n",
        "      token_max = 4000,\n",
        "  )\n",
        "\n",
        "  # Map/Reduce\n",
        "  \"\"\"\n",
        "  Uilizar MapReduceDocumentsChain\n",
        "  \"\"\"\n",
        "\n",
        "  map_reduce_chain = MapReduceDocumentsChain(\n",
        "    llm_chain=map_chain,\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    document_variable_name=\"character\",\n",
        "    return_intermediate_steps=False,\n",
        "  )\n",
        "\n",
        "\n",
        "  # Text splitter\n",
        "  \"\"\"\n",
        "  Usar RecursiveCharacterTextSplitter\n",
        "  \"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 15000,\n",
        "      chunk_overlap = 0\n",
        "  )\n",
        "\n",
        "  split_script = text_splitter.split_text(script)\n",
        "  split_script = text_splitter.create_documents(split_script)\n",
        "\n",
        "  # resultado\n",
        "  result = map_reduce_chain.invoke(split_script)\n",
        "  return result[\"output_text\"]\n",
        "\n",
        "\n",
        "# Formato del perfil\n",
        "def format_profile(answer_character_profile):\n",
        "  \"\"\"\n",
        "  Crear un json con las caracteristicas y que retorne\n",
        "  (final_profile, stats) del personaje\n",
        "  \"\"\"\n",
        "  match = re.search(\"\\*\\*\\*\", answer_character_profile)\n",
        "  json_text = answer_character_profile[match.end():].strip()\n",
        "  json_text = re.sub(r'\\s+', ' ', answer_character_list[match.end():].strip()).strip()\n",
        "  json_text = re.sub(r'\\`', ' ', json_text).strip()\n",
        "  json_text = re.sub(r'json', ' ', json_text).strip()\n",
        "\n",
        "  stats = json.loads(json_text)\n",
        "  final_profile = answer_character_profile[:match.start()]\n",
        "  return (final_profile, stats[0])\n"
      ],
      "metadata": {
        "id": "ha1zVrtkNaF-"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_character_list = map_reduce_text(\n",
        "    dune2_text,\n",
        "    'Paul Atreides'\n",
        ")"
      ],
      "metadata": {
        "id": "U5jl1RA_ZhzJ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_profile, stats = format_profile(answer_character_list)"
      ],
      "metadata": {
        "id": "0oduYAOZPaL_"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_profile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jzu-Qd_ts0j",
        "outputId": "99bc7310-5adb-4db5-a5bd-8825a15814db"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Character Attribute Analysis: Paul Atreides\n",
            "\n",
            "Here's a breakdown of Paul's attributes on a scale of 1 to 10, with explanations for each rating:\n",
            "\n",
            "**Intelligence:** 9\n",
            "\n",
            "Paul demonstrates a remarkable understanding of complex political machinations, strategic planning, and cultural nuances. He quickly grasps the importance of spice control, the power dynamics of the Great Houses, and the Fremen culture. His ability to anticipate threats, manipulate others, and devise effective strategies demonstrates a sharp and strategic mind. \n",
            "\n",
            "**Charisma:** 8\n",
            "\n",
            "Paul possesses a natural charisma that attracts followers and inspires loyalty. He commands respect through his confident leadership, bold declarations, and unwavering resolve. He uses his charm and manipulation skills to influence others, gaining their trust and support.\n",
            "\n",
            "**Strength:** 7\n",
            "\n",
            "Paul's physical prowess is demonstrated through his fighting skills, evident in his duel with Feyd-Rautha. However, his strength also lies in his mental fortitude and unwavering willpower. He remains composed under immense pressure, enduring loss and betrayal with remarkable resilience.\n",
            "\n",
            "**Wisdom:** 8\n",
            "\n",
            "Paul demonstrates a deep understanding of power, strategic foresight, and the importance of controlling resources. He strategically leverages his knowledge of Fremen culture and customs to his advantage. He also recognizes the potential consequences of his actions and plans accordingly. \n",
            "\n",
            "**Emotional Resilience:** 8\n",
            "\n",
            "Paul faces immense challenges, including loss, betrayal, and the burden of his visions. He channels his grief into a powerful drive for revenge and a commitment to his new people. Despite facing overwhelming odds, he remains determined and focused on his goals, showcasing remarkable resilience.\n",
            "\n",
            "**Creativity:** 7\n",
            "\n",
            "Paul demonstrates creativity through his innovative strategies, combining traditional Fremen tactics with his own unique approaches. He adapts to changing circumstances, learning from his mistakes and finding new ways to achieve his objectives. His ability to think outside the box and devise effective solutions showcases his creative thinking.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para gráficar stats. No Tocar.\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "def plot_stats(stats, character_name=\"Paul Atreides\"):\n",
        "    base_stats = [\n",
        "        \"Intelligence\", \"Charisma\", \"Strength\",\n",
        "        \"Wisdom\", \"Emotional Resilience\", \"Creativity\"\n",
        "    ]\n",
        "    for stat in base_stats:\n",
        "        if stat not in stats:\n",
        "            stats[stat] = 0\n",
        "\n",
        "    labels = list(stats.keys())\n",
        "    stats_values = list(stats.values())\n",
        "    stats_values += stats_values[:1]\n",
        "    labels += labels[:1]\n",
        "\n",
        "    # Plotly figure\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=stats_values,\n",
        "        theta=labels,\n",
        "        fill='toself',\n",
        "        name=character_name\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        polar=dict(\n",
        "            radialaxis=dict(\n",
        "                visible=True,\n",
        "                range=[0, max(stats_values)]\n",
        "            )\n",
        "        ),\n",
        "        showlegend=False,\n",
        "        title=character_name\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "fig = plot_stats(stats)"
      ],
      "metadata": {
        "id": "IXAHPyRSP6HY"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "f88RQHTlhCJa",
        "outputId": "81289fa7-37e9-4d2b-de22-b9f4a24eca80"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"6027cc73-e697-4691-9b1e-9f22cbd06ebd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6027cc73-e697-4691-9b1e-9f22cbd06ebd\")) {                    Plotly.newPlot(                        \"6027cc73-e697-4691-9b1e-9f22cbd06ebd\",                        [{\"fill\":\"toself\",\"name\":\"Paul Atreides\",\"r\":[9,8,7,8,8,7,9],\"theta\":[\"Intelligence\",\"Charisma\",\"Strength\",\"Wisdom\",\"Emotional Resilience\",\"Creativity\",\"Intelligence\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"radialaxis\":{\"visible\":true,\"range\":[0,9]}},\"showlegend\":false,\"title\":{\"text\":\"Paul Atreides\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6027cc73-e697-4691-9b1e-9f22cbd06ebd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Comentar (0.25 puntos)**\n",
        "\n",
        "En primer lugar, se utiliza un prompt para resumir el script, se especifica un enfoque particular para resolver la pregunta en cuestión.\n",
        "\n",
        "Para la solución del problema utilizamos un prompt que indicaba especificamente una serie de caracteres para identificar el resumen del json. Facilitando la extracción respectiva.\n",
        "\n",
        "1. ¿Qué otras tareas se podría realizar? De dos ejemplos con la metodología asociada.\n",
        "\n",
        "Dado que la respuesta del modelo es Resumen *** json. Se puede utilizar para cualquier problema que requiera un conjunto de datos estandarizados.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "\n",
        "*   Analisis de género de la pelicula: Se podría usar un prompt que analice los distintos géneros de la pelicula y devuelta un json con un puntaje de proporción de cada genero. (Una pelicula con genero romantica puede tener romance, pero no ser especificamente de romance)\n",
        "*   Lineas de participación de personajes: Similar a unos de los problemas anteriores, se puede hacer un conteo de la participación, a nivel de lineas en el script, para cada personaje.\n",
        "\n",
        "\n",
        "2. ¿Cual es la importancia de los prompt y como estos afectan al desempeño de los LLM?\n",
        "\n",
        "Los prompts guián al modelo para genear respuestas específicas. La idea es tener un prompt claro para reducir ambuguedades y mejorar precisión.\n",
        "\n",
        "3. ¿Alguna de sus respuestas fue una 'alucinación'? ¿Por qué sucede esto?\n",
        "\n",
        "Cuando utilizamos el modelo para generar un resumen del script de Dune parte 2 el modelo utilizó la información que conocía previamente, ya que el resumen generado para Dune parte 2 correspondia a un resumen de Dune parte 1.\n",
        "\n",
        "Según lo investigado (chatGPT), esto pudo ocurrir porque el modelo debe haber prioirizado información ya conocida sobre Dune parte 1 en lugar de enforcarse en el nuevo guión proporcionado.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_APhHBPXQXTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Reinforcement Learning (2.0 puntos)**\n",
        "\n",
        "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
      ],
      "metadata": {
        "id": "0hmHHQ9BuyAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq gymnasium stable_baselines3\n",
        "!pip install -qqq swig\n",
        "!pip install -qqq gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOcejYb6uzOO",
        "outputId": "3f20889e-0861-4d6a-f611-69b101993c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Blackjack (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Joaquín es fanático del Blackjack, por lo que en esta subsección implementarán métodos de RL y así generar una estrategia para que pueda ~~ir al casino a  hacerse millonario~~ aprender a resolver problemas mediante RL.\n",
        "\n",
        "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
      ],
      "metadata": {
        "id": "qBPet_Mq8dX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)"
      ],
      "metadata": {
        "id": "LpZ8bBKk9ZlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.1 Descripción de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
      ],
      "metadata": {
        "id": "ZJ6J1_-Y9nHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El ambiente de Blackjack en Gymnasium modela el juego de cartas Blackjack usando un Proceso de Decisión de Markov (MDP).\n",
        "\n",
        "•\tEstados: Consisten en una tupla de tres elementos: la suma actual del jugador, el valor de la carta visible del crupier (1-10), y si el jugador tiene un as utilizable (0 o 1).\n",
        "\n",
        "•\tAcciones: Discretas, con dos posibles: pedir carta (hit) o plantarse (stick).\n",
        "\n",
        "•\tRecompensas: +1 por ganar, -1 por perder, 0 por empatar y +1.5 por ganar con un Blackjack natural."
      ],
      "metadata": {
        "id": "G5i1Wt1p770x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "* Simule un escenario en donde se escojan acciones aleatorias. Repita esta\n",
        "simulación 5000 veces y reporte el promedio y desviación de las recompensas.\n",
        "* ¿Cómo calificaría el performance de esta política?\n",
        "* ¿Cómo podría interpretar las recompensas obtenidas?"
      ],
      "metadata": {
        "id": "pmcX6bRC9agQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "# Simular 5000 episodios con acciones aleatorias\n",
        "n_episodes = 5000\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    observation, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Acción aleatoria\n",
        "        observation, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviación estándar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "id": "RHCfKN7NGi1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e169de94-6550-4da2-d9cd-4dd70b8287e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -0.387\n",
            "Desviación estándar de las recompensas: 0.9017932135473187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. El performance de la política aleatoria es bajo. El promedio de recompensas de -0.387 indica que, en promedio, la política aleatoria tiende a perder más juegos de los que gana.\n",
        "\n",
        "2. Desviación Estándar (0.9018): Una desviación estándar relativamente alta muestra que los resultados de los episodios son variables, lo que es esperado con una política aleatoria, ya que las decisiones no siguen una estrategia coherente."
      ],
      "metadata": {
        "id": "nuKbcR2RCbVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
      ],
      "metadata": {
        "id": "LEO_dY4x_SJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete([32, 11, 2])\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Crear el modelo A2C\n",
        "model = A2C(\"MlpPolicy\", env, verbose=1, seed=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=50000)\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"a2c_blackjack\")\n",
        "\n",
        "# Cargar el modelo\n",
        "model = A2C.load(\"a2c_blackjack\")"
      ],
      "metadata": {
        "id": "T0sp8XWsGg4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c149f8c-47f5-4b3d-9b7e-48ff65295a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 212      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.137   |\n",
            "|    explained_variance | 0.24     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 0.00521  |\n",
            "|    value_loss         | 0.737    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 278      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.144   |\n",
            "|    explained_variance | 0.741    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -0.00609 |\n",
            "|    value_loss         | 0.105    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 311      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0459  |\n",
            "|    explained_variance | -0.264   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 0.00605  |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 332      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0555  |\n",
            "|    explained_variance | 0.214    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 0.00258  |\n",
            "|    value_loss         | 0.909    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 346      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.208   |\n",
            "|    explained_variance | 0.0625   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 0.00687  |\n",
            "|    value_loss         | 0.774    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 355      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.206   |\n",
            "|    explained_variance | -0.0357  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -0.77    |\n",
            "|    value_loss         | 0.852    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 362      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.344    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -0.0117  |\n",
            "|    value_loss         | 0.641    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 367      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.251   |\n",
            "|    explained_variance | 0.308    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -0.0515  |\n",
            "|    value_loss         | 0.912    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 365      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.615   |\n",
            "|    explained_variance | -1.12    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -0.233   |\n",
            "|    value_loss         | 0.332    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 355      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | -0.232   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 0.0641   |\n",
            "|    value_loss         | 0.786    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 354      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.197   |\n",
            "|    explained_variance | 0.101    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -0.0813  |\n",
            "|    value_loss         | 0.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 359      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.114   |\n",
            "|    explained_variance | 0.851    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -0.0105  |\n",
            "|    value_loss         | 0.204    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 361      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | -0.231   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -0.25    |\n",
            "|    value_loss         | 1.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 365      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.169   |\n",
            "|    explained_variance | 0.626    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -0.00686 |\n",
            "|    value_loss         | 0.132    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 368      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0886  |\n",
            "|    explained_variance | -0.117   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 0.044    |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 371      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.225   |\n",
            "|    explained_variance | -0.48    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 1.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.15    |\n",
            "|    explained_variance | 0.156    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 0.699    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0983  |\n",
            "|    explained_variance | 0.463    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -0.0139  |\n",
            "|    value_loss         | 0.393    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.19     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 0.0689   |\n",
            "|    value_loss         | 0.461    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 372      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | -2.39    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 0.0571   |\n",
            "|    value_loss         | 0.967    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 368      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | -254     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | -0.174   |\n",
            "|    value_loss         | 0.279    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 370       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.125    |\n",
            "|    explained_variance | -3.68e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | -0.132    |\n",
            "|    value_loss         | 0.461     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 372      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.106   |\n",
            "|    explained_variance | 0.247    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | -0.0359  |\n",
            "|    value_loss         | 0.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0564  |\n",
            "|    explained_variance | 0.00232  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 0.0209   |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0519  |\n",
            "|    explained_variance | -0.416   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -0.00129 |\n",
            "|    value_loss         | 0.288    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.162   |\n",
            "|    explained_variance | 0.671    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -0.0485  |\n",
            "|    value_loss         | 0.398    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 378       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0101   |\n",
            "|    explained_variance | 0.603     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -0.000861 |\n",
            "|    value_loss         | 0.237     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.839    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -0.0878  |\n",
            "|    value_loss         | 0.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 38       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.181   |\n",
            "|    explained_variance | 0.645    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -0.0419  |\n",
            "|    value_loss         | 0.342    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0467  |\n",
            "|    explained_variance | -1.31    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -0.00783 |\n",
            "|    value_loss         | 1.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 41       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.363    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 0.527    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00473 |\n",
            "|    explained_variance | 0.276    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 0.00033  |\n",
            "|    value_loss         | 0.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 43       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0191  |\n",
            "|    explained_variance | 0.752    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 0.000846 |\n",
            "|    value_loss         | 0.148    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.000985 |\n",
            "|    explained_variance | 0.79      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | -4.16e-05 |\n",
            "|    value_loss         | 0.208     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 46       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0447  |\n",
            "|    explained_variance | -0.0418  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 0.014    |\n",
            "|    value_loss         | 0.667    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 47       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.043   |\n",
            "|    explained_variance | 0.36     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | -0.00326 |\n",
            "|    value_loss         | 0.666    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 381      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 48       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0153  |\n",
            "|    explained_variance | -39.1    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | -0.00103 |\n",
            "|    value_loss         | 0.544    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 382      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 49       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.126   |\n",
            "|    explained_variance | -1.34    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 0.0263   |\n",
            "|    value_loss         | 0.822    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 383       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.12     |\n",
            "|    explained_variance | 0.168     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -0.000233 |\n",
            "|    value_loss         | 0.554     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 383      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 52       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0548  |\n",
            "|    explained_variance | 0.593    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -0.00835 |\n",
            "|    value_loss         | 0.282    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 53       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.111   |\n",
            "|    explained_variance | 0.252    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | -0.304   |\n",
            "|    value_loss         | 0.484    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0833  |\n",
            "|    explained_variance | 0.662    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | -0.032   |\n",
            "|    value_loss         | 0.792    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 56       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0774  |\n",
            "|    explained_variance | -11.4    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -0.0164  |\n",
            "|    value_loss         | 0.452    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 381      |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 57       |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.13    |\n",
            "|    explained_variance | -0.265   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | -0.0316  |\n",
            "|    value_loss         | 1.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 382      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 58       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.147   |\n",
            "|    explained_variance | 0.243    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | -0.125   |\n",
            "|    value_loss         | 0.511    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 383      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 59       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0413  |\n",
            "|    explained_variance | 0.0998   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 0.00254  |\n",
            "|    value_loss         | 0.597    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 384      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 61       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.311    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 0.661    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 385      |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 62       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.164   |\n",
            "|    explained_variance | -0.173   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | -0.0681  |\n",
            "|    value_loss         | 1.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 385      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 63       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0326  |\n",
            "|    explained_variance | 0.259    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 0.00472  |\n",
            "|    value_loss         | 0.415    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 385      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 64       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0631  |\n",
            "|    explained_variance | 0.675    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -0.00754 |\n",
            "|    value_loss         | 0.451    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 382       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 66        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0364   |\n",
            "|    explained_variance | -3.78e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -0.00457  |\n",
            "|    value_loss         | 0.579     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 68        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0135   |\n",
            "|    explained_variance | 0.662     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -0.000894 |\n",
            "|    value_loss         | 0.324     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 70       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0519  |\n",
            "|    explained_variance | 0.703    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -0.00692 |\n",
            "|    value_loss         | 0.294    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 375       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 71        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0294   |\n",
            "|    explained_variance | 0.546     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -0.000736 |\n",
            "|    value_loss         | 0.524     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 73       |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0574  |\n",
            "|    explained_variance | 0.919    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 0.00115  |\n",
            "|    value_loss         | 0.0851   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 74       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.12     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -0.0171  |\n",
            "|    value_loss         | 0.531    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 75        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0055   |\n",
            "|    explained_variance | 0.792     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -0.000402 |\n",
            "|    value_loss         | 0.249     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 76       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0424  |\n",
            "|    explained_variance | -0.437   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -0.00865 |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 78       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0102  |\n",
            "|    explained_variance | 0.567    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | 0.00207  |\n",
            "|    value_loss         | 0.278    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 79       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0283  |\n",
            "|    explained_variance | 0.64     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | -0.245   |\n",
            "|    value_loss         | 0.236    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 374       |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 81        |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00106  |\n",
            "|    explained_variance | -1.66e+04 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 0.000144  |\n",
            "|    value_loss         | 1         |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 372      |\n",
            "|    iterations         | 6200     |\n",
            "|    time_elapsed       | 83       |\n",
            "|    total_timesteps    | 31000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.156   |\n",
            "|    explained_variance | 0.357    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6199     |\n",
            "|    policy_loss        | 0.0803   |\n",
            "|    value_loss         | 0.888    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 372      |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 84       |\n",
            "|    total_timesteps    | 31500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0416  |\n",
            "|    explained_variance | 0.696    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | -0.00392 |\n",
            "|    value_loss         | 0.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 373      |\n",
            "|    iterations         | 6400     |\n",
            "|    time_elapsed       | 85       |\n",
            "|    total_timesteps    | 32000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0693  |\n",
            "|    explained_variance | 0.0997   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6399     |\n",
            "|    policy_loss        | 0.00103  |\n",
            "|    value_loss         | 1.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 6500     |\n",
            "|    time_elapsed       | 86       |\n",
            "|    total_timesteps    | 32500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0783  |\n",
            "|    explained_variance | 0.347    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6499     |\n",
            "|    policy_loss        | 0.00109  |\n",
            "|    value_loss         | 0.657    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 6600     |\n",
            "|    time_elapsed       | 87       |\n",
            "|    total_timesteps    | 33000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0518  |\n",
            "|    explained_variance | 0.129    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6599     |\n",
            "|    policy_loss        | 0.00971  |\n",
            "|    value_loss         | 0.965    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 6700     |\n",
            "|    time_elapsed       | 89       |\n",
            "|    total_timesteps    | 33500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.124   |\n",
            "|    explained_variance | -0.24    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6699     |\n",
            "|    policy_loss        | 0.0632   |\n",
            "|    value_loss         | 0.413    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 6800     |\n",
            "|    time_elapsed       | 90       |\n",
            "|    total_timesteps    | 34000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.193   |\n",
            "|    explained_variance | 0.385    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6799     |\n",
            "|    policy_loss        | -0.00822 |\n",
            "|    value_loss         | 0.117    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 91        |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00525  |\n",
            "|    explained_variance | 0.225     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | -0.000583 |\n",
            "|    value_loss         | 0.837     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 7000      |\n",
            "|    time_elapsed       | 92        |\n",
            "|    total_timesteps    | 35000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00324  |\n",
            "|    explained_variance | -2.11e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6999      |\n",
            "|    policy_loss        | 0.000598  |\n",
            "|    value_loss         | 1.45      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 376       |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 94        |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0229   |\n",
            "|    explained_variance | -3.49e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 0.00814   |\n",
            "|    value_loss         | 1.24      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 7200     |\n",
            "|    time_elapsed       | 95       |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0653  |\n",
            "|    explained_variance | 0.199    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | -0.00747 |\n",
            "|    value_loss         | 0.985    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 7300     |\n",
            "|    time_elapsed       | 97       |\n",
            "|    total_timesteps    | 36500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.138   |\n",
            "|    explained_variance | 0.319    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7299     |\n",
            "|    policy_loss        | -0.0646  |\n",
            "|    value_loss         | 1.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 7400     |\n",
            "|    time_elapsed       | 98       |\n",
            "|    total_timesteps    | 37000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0162  |\n",
            "|    explained_variance | 0.592    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | 0.00047  |\n",
            "|    value_loss         | 0.371    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 99       |\n",
            "|    total_timesteps    | 37500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00165 |\n",
            "|    explained_variance | 0.205    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | 0.000112 |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00238  |\n",
            "|    explained_variance | 0.619     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | -0.000135 |\n",
            "|    value_loss         | 0.213     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 7700     |\n",
            "|    time_elapsed       | 101      |\n",
            "|    total_timesteps    | 38500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00281 |\n",
            "|    explained_variance | 0.409    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7699     |\n",
            "|    policy_loss        | 0.000181 |\n",
            "|    value_loss         | 0.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 7800     |\n",
            "|    time_elapsed       | 102      |\n",
            "|    total_timesteps    | 39000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0321  |\n",
            "|    explained_variance | 0.778    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7799     |\n",
            "|    policy_loss        | -0.00475 |\n",
            "|    value_loss         | 0.213    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 104      |\n",
            "|    total_timesteps    | 39500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.134   |\n",
            "|    explained_variance | -0.103   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | 0.121    |\n",
            "|    value_loss         | 0.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 8000     |\n",
            "|    time_elapsed       | 105      |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0566  |\n",
            "|    explained_variance | 0.668    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | -0.00425 |\n",
            "|    value_loss         | 0.243    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 107      |\n",
            "|    total_timesteps    | 40500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00171 |\n",
            "|    explained_variance | -0.141   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | 0.000256 |\n",
            "|    value_loss         | 0.909    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 8200     |\n",
            "|    time_elapsed       | 108      |\n",
            "|    total_timesteps    | 41000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0425  |\n",
            "|    explained_variance | 0.281    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8199     |\n",
            "|    policy_loss        | 0.0126   |\n",
            "|    value_loss         | 0.879    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 376       |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 110       |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.000343 |\n",
            "|    explained_variance | -3.72     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 2.78e-05  |\n",
            "|    value_loss         | 1.32      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 8400     |\n",
            "|    time_elapsed       | 111      |\n",
            "|    total_timesteps    | 42000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00352 |\n",
            "|    explained_variance | -0.211   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8399     |\n",
            "|    policy_loss        | 0.000606 |\n",
            "|    value_loss         | 0.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 112      |\n",
            "|    total_timesteps    | 42500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0963  |\n",
            "|    explained_variance | 0.167    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | -0.0117  |\n",
            "|    value_loss         | 0.208    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 8600     |\n",
            "|    time_elapsed       | 113      |\n",
            "|    total_timesteps    | 43000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.106   |\n",
            "|    explained_variance | 0.604    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8599     |\n",
            "|    policy_loss        | -0.0181  |\n",
            "|    value_loss         | 0.216    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 8700     |\n",
            "|    time_elapsed       | 114      |\n",
            "|    total_timesteps    | 43500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0821  |\n",
            "|    explained_variance | 0.396    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8699     |\n",
            "|    policy_loss        | 0.0273   |\n",
            "|    value_loss         | 0.499    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 8800     |\n",
            "|    time_elapsed       | 116      |\n",
            "|    total_timesteps    | 44000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.071   |\n",
            "|    explained_variance | 0.145    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8799     |\n",
            "|    policy_loss        | -0.00553 |\n",
            "|    value_loss         | 0.839    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 8900     |\n",
            "|    time_elapsed       | 117      |\n",
            "|    total_timesteps    | 44500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00012 |\n",
            "|    explained_variance | 0.029    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8899     |\n",
            "|    policy_loss        | 1.13e-05 |\n",
            "|    value_loss         | 0.551    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9000     |\n",
            "|    time_elapsed       | 118      |\n",
            "|    total_timesteps    | 45000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.102   |\n",
            "|    explained_variance | 0.269    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8999     |\n",
            "|    policy_loss        | 0.432    |\n",
            "|    value_loss         | 0.797    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9100     |\n",
            "|    time_elapsed       | 119      |\n",
            "|    total_timesteps    | 45500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0502  |\n",
            "|    explained_variance | 0.158    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9099     |\n",
            "|    policy_loss        | 0.0174   |\n",
            "|    value_loss         | 0.545    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 379       |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 121       |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0004   |\n",
            "|    explained_variance | 0.675     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | -2.54e-05 |\n",
            "|    value_loss         | 0.278     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 9300     |\n",
            "|    time_elapsed       | 122      |\n",
            "|    total_timesteps    | 46500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0181  |\n",
            "|    explained_variance | 0.283    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9299     |\n",
            "|    policy_loss        | -0.00438 |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 378       |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 124       |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.000175 |\n",
            "|    explained_variance | 0.363     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 1.3e-05   |\n",
            "|    value_loss         | 0.446     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 125      |\n",
            "|    total_timesteps    | 47500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0822  |\n",
            "|    explained_variance | -0.099   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | -0.00546 |\n",
            "|    value_loss         | 0.267    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 379       |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 126       |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0248   |\n",
            "|    explained_variance | -2.22e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | -0.721    |\n",
            "|    value_loss         | 0.605     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 127      |\n",
            "|    total_timesteps    | 48500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.05    |\n",
            "|    explained_variance | 0.874    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | -0.00564 |\n",
            "|    value_loss         | 0.221    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 128      |\n",
            "|    total_timesteps    | 49000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0422  |\n",
            "|    explained_variance | 0.0733   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 0.0111   |\n",
            "|    value_loss         | 0.926    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9900     |\n",
            "|    time_elapsed       | 129      |\n",
            "|    total_timesteps    | 49500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.147   |\n",
            "|    explained_variance | -5.27    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9899     |\n",
            "|    policy_loss        | -0.0345  |\n",
            "|    value_loss         | 0.675    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 381       |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 131       |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0113   |\n",
            "|    explained_variance | 0.21      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | -0.000889 |\n",
            "|    value_loss         | 0.641     |\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.4 Evaluación de modelo (0.2 puntos)**\n",
        "\n",
        "* Repita el ejercicio 2.1.2 pero utilizando el modelo entrenado.\n",
        "* ¿Cómo es el performance de su agente?\n",
        "* ¿Es mejor o peor que el escenario baseline?"
      ],
      "metadata": {
        "id": "E-bpdb8wZID1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete([32, 11, 2])\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = A2C.load(\"a2c_blackjack\")\n",
        "\n",
        "# Simular 5000 episodios con el modelo entrenado\n",
        "n_episodes = 5000\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward[0]\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviación estándar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "id": "S7jdmnTwGePD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2537123f-47b2-4edd-f0de-463009013cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -0.0712\n",
            "Desviación estándar de las recompensas: 0.9466417273710259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparado con el escenario baseline, donde el promedio de recompensas era -0.387, el agente entrenado muestra una mejora, ya que la pérdida promedio es menor. Esto sugiere que el agente ha aprendido a tomar decisiones más efectivas que una política aleatoria."
      ],
      "metadata": {
        "id": "IUXYAG_jGWTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.5 Estudio de acciones (0.2 puntos)**\n",
        "\n",
        "* Genere una función que reciba un estado y retorne la accion del agente.\n",
        "* Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
        "\n",
        "  * Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
        "  * Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
        "\n",
        "* ¿Son coherentes sus acciones con las reglas del juego?\n",
        "\n",
        "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
      ],
      "metadata": {
        "id": "RO-EsAaPAYEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# escriba su respuesta acá\n",
        "# Función para predecir la acción\n",
        "def predict_action(state):\n",
        "    state = np.array(state).flatten().reshape(1, -1)\n",
        "    action, _ = model.predict(state, deterministic=True)\n",
        "    return action\n",
        "\n",
        "# Escenarios dados\n",
        "state1 = (6, 7, 0)  # Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene un as\n",
        "state2 = (19, 3, 1)  # Suma de cartas del agente es 19, dealer muestra un 3, agente tiene un as\n",
        "\n",
        "# Obtener las acciones\n",
        "action1 = predict_action(state1)\n",
        "action2 = predict_action(state2)\n",
        "\n",
        "print(f\"Acción para el estado 1: {action1}\")\n",
        "print(f\"Acción para el estado 2: {action2}\")"
      ],
      "metadata": {
        "id": "Lssdp7AvGaRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008845f1-dd85-421b-9967-ef593e162153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acción para el estado 1: [1]\n",
            "Acción para el estado 2: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\tEstado 1: Acción [1] corresponde a “pedir carta”. Con una suma de 6 y el crupier mostrando un 7, es coherente que el agente pida carta, ya que la suma es muy baja.\n",
        "\n",
        "•\tEstado 2: Acción [0] corresponde a “plantarse”. Con una suma de 19 y el crupier mostrando un 3, es coherente plantarse, ya que la suma es alta y el riesgo de pasarse es considerable.\n",
        "\n",
        "Ambas acciones son coherentes con las estrategias típicas en Blackjack."
      ],
      "metadata": {
        "id": "gRXVON2WG0UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 LunarLander**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n"
      ],
      "metadata": {
        "id": "SEqCTqqroh03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.1 Descripción de MDP (0.2 puntos)**\n"
      ],
      "metadata": {
        "id": "sk5VJVppXh3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comencemos preparando el ambiente:"
      ],
      "metadata": {
        "id": "XvAVq1wQIjLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
      ],
      "metadata": {
        "id": "Qb5PmadJIngR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas.\n",
        "* ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
        "* En la preparación del ambiente se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?"
      ],
      "metadata": {
        "id": "LNERH-m8JYQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descripción del ambiente Lunar Lander:\n",
        "\n",
        "•\tEstados: Es un vector de 8 dimensiones que incluye coordenadas, velocidades, ángulo, velocidad angular y contacto de las patas con el suelo.\n",
        "•\tAcciones: Cuatro acciones discretas (sin motor, motor principal, motor lateral izquierdo y derecho).\n",
        "•\tRecompensas: Basadas en la proximidad al objetivo, velocidad, inclinación, contacto de las patas y el uso de motores.\n",
        "\n",
        "Comparación con Blackjack:\n",
        "\n",
        "En Lunar Lander, las acciones son más complejas, involucrando control de motores y orientación, mientras que en Blackjack solo son decisiones de “pedir” o “plantarse”.\n",
        "\n",
        "Implicancias de continuous = True:\n",
        "\n",
        "Cuando continuous = True, las acciones se vuelven continuas, permitiendo un control más preciso del motor principal y los motores laterales, en lugar de simplemente encender o apagar motores."
      ],
      "metadata": {
        "id": "DbpGahPcHAje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "* Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas.\n",
        "* ¿Cómo calificaría el performance de esta política?"
      ],
      "metadata": {
        "id": "YChodtNQwzG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Crear el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\", continuous=True)\n",
        "\n",
        "# Simular 10 episodios con acciones aleatorias\n",
        "n_episodes = 10\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    obs, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Acción aleatoria\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviación estándar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "id": "pNMT_GORIreW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e41e3ef-6a08-4597-d8f4-a716ee2e3f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -246.5308163586496\n",
            "Desviación estándar de las recompensas: 211.09166153352746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El performance de la política aleatoria es bajo, con un promedio de recompensas de aproximadamente -246.53. Esto indica que, en general, el agente tiene dificultades para aterrizar correctamente. La alta desviación estándar de 211.09 sugiere una gran variabilidad en los resultados, lo que es típico de una política que no toma decisiones informadas."
      ],
      "metadata": {
        "id": "d1n-P5XHH6DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "* A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
      ],
      "metadata": {
        "id": "hQrZVQflX_5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Crear el modelo DDPG\n",
        "model = DDPG(\"MlpPolicy\", env, verbose=1, seed=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"ddpg_lunar_lander\")\n",
        "\n",
        "# Cargar el modelo\n",
        "model = DDPG.load(\"ddpg_lunar_lander\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Mg0epSnLKfy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7fdd28-4a0e-41a2-a5d2-d68042bb8fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 7        |\n",
            "|    total_timesteps | 381      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 10.7     |\n",
            "|    critic_loss     | 21.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 280      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 38       |\n",
            "|    time_elapsed    | 17       |\n",
            "|    total_timesteps | 683      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 23.4     |\n",
            "|    critic_loss     | 22.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 582      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 62       |\n",
            "|    total_timesteps | 1967     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 22.2     |\n",
            "|    critic_loss     | 25       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1866     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 30       |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 2625     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.1     |\n",
            "|    critic_loss     | 39.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 2524     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 30       |\n",
            "|    time_elapsed    | 112      |\n",
            "|    total_timesteps | 3472     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 14.1     |\n",
            "|    critic_loss     | 39.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3371     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 145      |\n",
            "|    total_timesteps | 4557     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.19     |\n",
            "|    critic_loss     | 9.88     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 4456     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 30       |\n",
            "|    time_elapsed    | 195      |\n",
            "|    total_timesteps | 6029     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -3.22    |\n",
            "|    critic_loss     | 29.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 5928     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 255      |\n",
            "|    total_timesteps | 7961     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.7    |\n",
            "|    critic_loss     | 20.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7860     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 270      |\n",
            "|    total_timesteps | 8434     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -30.1    |\n",
            "|    critic_loss     | 26.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8333     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 298      |\n",
            "|    total_timesteps | 9347     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.1    |\n",
            "|    critic_loss     | 19.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9246     |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.4 Evaluación de modelo (0.2 puntos)**\n",
        "\n",
        "* Repita el ejercicio 2.2.2 pero utilizando el modelo entrenado.\n",
        "* ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
      ],
      "metadata": {
        "id": "3z-oIUSrlAsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = DDPG.load(\"ddpg_lunar_lander\")\n",
        "\n",
        "# Simular 10 episodios con el modelo entrenado\n",
        "n_episodes = 10\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward[0]\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviación estándar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "id": "CWVY1a39KeRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8f40c8-1d45-4d46-d424-fe22247f77ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -76.05608874436003\n",
            "Desviación estándar de las recompensas: 48.65351366634402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance del Agente:\n",
        "El agente entrenado tiene un promedio de recompensas de -76.06 con una desviación estándar de 48.65. Esto muestra una mejora significativa respecto al baseline.\n",
        "\n",
        "Comparación con el Escenario Baseline:\n",
        "Comparado con el baseline, donde el promedio de recompensas era -246.53, el agente es claramente mejor. Las recompensas más altas y la menor desviación estándar indican un comportamiento más consistente y efectivo."
      ],
      "metadata": {
        "id": "RZe251MWUyrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.5 Optimización de modelo (0.2 puntos)**\n",
        "\n",
        "* Repita los ejercicios 2.2.3 y 2.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
        "  - `total_timesteps`\n",
        "  - `learning_rate`\n",
        "  - `batch_size`\n",
        "\n",
        "* Una vez optimizado el modelo, use la función `export_gif` entregada para estudiar el comportamiento de su agente en la resolución del ambiente, comente sobre sus resultados.\n",
        "\n",
        "* Adjunte el gif generado en su entrega. Si, además, adjuntan el gif en el markdown tendrán un bonus de 0.1."
      ],
      "metadata": {
        "id": "x6Xw4YHT3P5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def export_gif(model, env, n=5):\n",
        "    '''\n",
        "    Función que exporta a gif el comportamiento del agente en n episodios.\n",
        "    '''\n",
        "    images = []\n",
        "    for episode in range(n):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            img = env.render(mode=\"rgb_array\")\n",
        "            if img is not None:\n",
        "                images.append(img)\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    if images:\n",
        "        imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i % 2 == 0], fps=29)\n"
      ],
      "metadata": {
        "id": "pwL5djUkMS3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creando un modelo que cambie los parametros.\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Ajustar parámetros del modelo\n",
        "model = DDPG(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    verbose=1,\n",
        "    seed=42,\n",
        "    learning_rate=0.0001,  # Ajusta el learning rate\n",
        "    batch_size=64         # Ajusta el batch size\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=20000)  # Aumenta el total_timesteps\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"ddpg_lunar_lander_optimized\")\n",
        "\n",
        "# Cargar el modelo\n",
        "model = DDPG.load(\"ddpg_lunar_lander_optimized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dv0q01RNXls",
        "outputId": "8d09cb28-1920-47b2-8bea-206350326f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 73       |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 354      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 5.97     |\n",
            "|    critic_loss     | 105      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 253      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 70       |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 669      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.45     |\n",
            "|    critic_loss     | 124      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 568      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 63       |\n",
            "|    time_elapsed    | 19       |\n",
            "|    total_timesteps | 1216     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 1.76     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1115     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 61       |\n",
            "|    time_elapsed    | 26       |\n",
            "|    total_timesteps | 1628     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 9.95     |\n",
            "|    critic_loss     | 72.8     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1527     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 2231     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.6     |\n",
            "|    critic_loss     | 6.31     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2130     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 44       |\n",
            "|    total_timesteps | 2690     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.8     |\n",
            "|    critic_loss     | 37.8     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2589     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 58       |\n",
            "|    total_timesteps | 3538     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 10.5     |\n",
            "|    critic_loss     | 1.2      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 3437     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 68       |\n",
            "|    total_timesteps | 4156     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 13.8     |\n",
            "|    critic_loss     | 3.34     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 4055     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 79       |\n",
            "|    total_timesteps | 4808     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 20.4     |\n",
            "|    critic_loss     | 47.4     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 4707     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 61       |\n",
            "|    time_elapsed    | 95       |\n",
            "|    total_timesteps | 5809     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.3     |\n",
            "|    critic_loss     | 9.04     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 5708     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 101      |\n",
            "|    total_timesteps | 6190     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 14.9     |\n",
            "|    critic_loss     | 41.7     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 6089     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 111      |\n",
            "|    total_timesteps | 6790     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 19.2     |\n",
            "|    critic_loss     | 9.21     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 6689     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 123      |\n",
            "|    total_timesteps | 7536     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 12       |\n",
            "|    critic_loss     | 65.1     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 7435     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 134      |\n",
            "|    total_timesteps | 8121     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 32       |\n",
            "|    critic_loss     | 14.2     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 8020     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 143      |\n",
            "|    total_timesteps | 8626     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 22       |\n",
            "|    critic_loss     | 9.74     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 8525     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 167      |\n",
            "|    total_timesteps | 10114    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18       |\n",
            "|    critic_loss     | 5.63     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 10013    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 177      |\n",
            "|    total_timesteps | 10785    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 22.8     |\n",
            "|    critic_loss     | 7.87     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 10684    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 193      |\n",
            "|    total_timesteps | 11734    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 25.7     |\n",
            "|    critic_loss     | 5.5      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 11633    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 212      |\n",
            "|    total_timesteps | 12866    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 10.4     |\n",
            "|    critic_loss     | 1.68     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 12765    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 224      |\n",
            "|    total_timesteps | 13566    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.43     |\n",
            "|    critic_loss     | 0.922    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 13465    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 238      |\n",
            "|    total_timesteps | 14313    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.4     |\n",
            "|    critic_loss     | 19.3     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 14212    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 259      |\n",
            "|    total_timesteps | 15650    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.4     |\n",
            "|    critic_loss     | 9.38     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 15549    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 272      |\n",
            "|    total_timesteps | 16445    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 13.7     |\n",
            "|    critic_loss     | 24.4     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 16344    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 287      |\n",
            "|    total_timesteps | 17363    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 16.5     |\n",
            "|    critic_loss     | 21.2     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 17262    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 311      |\n",
            "|    total_timesteps | 18784    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.47     |\n",
            "|    critic_loss     | 1.8      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 18683    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 328      |\n",
            "|    total_timesteps | 19844    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.43     |\n",
            "|    critic_loss     | 2.49     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 19743    |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Cargar el modelo\n",
        "model = DDPG.load(\"ddpg_lunar_lander_optimized\")\n",
        "\n",
        "# Simular 10 episodios con el modelo entrenado\n",
        "n_episodes = 10\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward[0]\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviación estándar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugtbmpp9NaBH",
        "outputId": "1272936f-41e1-4d33-c879-7f6de27315d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -58.281686818308664\n",
            "Desviación estándar de las recompensas: 64.18256240944172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#usando la función\n",
        "export_gif(model, env, n=5)"
      ],
      "metadata": {
        "id": "VvSTimK0SIuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BALqZZGfUkx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}