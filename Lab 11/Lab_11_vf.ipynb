{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**<h1><center>Laboratorio 11: LLM y Agentes Aut칩nomos 游뱄</center></h1>**\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos</strong></center>"
      ],
      "metadata": {
        "id": "PyPTffTLug7i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD8X1uhGzAHq",
        "cell_id": "737a4540885f41acb34b9863a968b907",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesor: Ignacio Meza, Sebastian Tinoco\n",
        "- Auxiliar: Catherine Benavides, Consuelo Rojas\n",
        "- Ayudante: Eduardo Moya, Nicol치s Ojeda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXflExjqzAHr",
        "cell_id": "e4a6f26138654eb49ee963fb4c7ecf46",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### Equipo: **SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados**\n",
        "\n",
        "- Nombre de alumno 1: Sergio Rehbein\n",
        "- Nombre de alumno 2: Mat칤as Cornejo\n",
        "\n",
        "**SUPER IMPORTANTE** - notebooks sin nombre no ser치n revisados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD-V0bbZzAHr",
        "owner_user_id": "badcc427-fd3d-4615-9296-faa43ec69cfb",
        "cell_id": "7dd4aaebd4f44063aedbb47ea36349a5",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "\\### **Link de repositorio de GitHub Matias:** https://github.com/s-kill/MDS7202\n",
        "\n",
        "\\### **Link de repositorio de GitHub Sergio:** https://github.com/sergiorehbein/MDS7201---Proyecto-de-Ciencia-de-Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcnsiQMkzAHr",
        "cell_id": "abe08e51696a471e8cc8ac1fa4216f0b",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### **Indice**\n",
        "\n",
        "1. [Temas a tratar](#Temas-a-tratar:)\n",
        "3. [Descripcci칩n del laboratorio](#Descripci칩n-del-laboratorio.)\n",
        "4. [Desarrollo](#Desarrollo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uBLPj1PzAHs",
        "cell_id": "0174e9377ebb43eaa0d12718db4c81ec",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## **Temas a tratar**\n",
        "\n",
        "- Implementaci칩n de modelos de LLM y Reinforcement Learning.\n",
        "- Utilizaci칩n e implementaci칩n de agentes.\n",
        "\n",
        "## **Reglas:**\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 7 d칤as desde la publicaci칩n, 3 d칤as de atraso con 1 punto de descuento c/u.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria.\n",
        "- Prohibidas las copias. Cualquier intento de copia ser치 debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est칠n en u-cursos no ser치n revisados. Recuerden que el repositorio tambi칠n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
        "Pueden usar cualquer material del curso que estimen conveniente.\n",
        "\n",
        "### **Objetivos principales del laboratorio**\n",
        "\n",
        "- Generar un modelo LLM generativo interactivo.\n",
        "- Entrenar un modelo de Reinforce Learning.\n",
        "\n",
        "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Large Language Models (4.0 puntos)**"
      ],
      "metadata": {
        "id": "Is4P4NDMurx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://trestristescriticos.com/wp-content/uploads/2021/07/telefono-gratuito-cinesur.jpg\" width=\"350\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "tJ3yV96HwN75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Joaqu칤n no es un aficionado del cine, pero a principios de a침o, se propuso ver m치s peliculas para poder tener m치s temas de conversaci칩n con sus amigos y familia. Sin embargo, ya es junio y Joaqu칤n no ha visto ninguna pelicula nueva o relevante de las que ten칤a en su lista y su reuni칩n familiar bi-anual se acerca y necesita la mayor informaci칩n que pueda recopilar de dichas peliculas sin tener que verlas.\n",
        "\n",
        "Para esto, usted con su compa침erx, tendr치 que crear una aplicaci칩n utilizando LangChain.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kB8z1qrGww4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instalaci칩n de librer칤as**\n",
        "\n",
        "Para la creaci칩n de la aplicaci칩n, se utilizara un modelo de lenguaje (LLM) ofrecido gratuitamente por Google.\n",
        "\n",
        "Para ello, se utilizar치 la API de Gemini, por lo que si no tienen acceso, se pueden crear una cuenta en el siguiente [enlace a Google AI](https://ai.google.dev/). Ah칤, ir a la pesta침a superior y seleccione la opci칩n que dice ``Gemini API``.\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-06-13_at_12.42.32_PM.png' width='450' />\n",
        "\n",
        "Luego, seleccione el bot칩n que dice ``Get API key in Google AI Studio`` y hacer click en ``Crear clave de API`` para generar la llave con la que se podr치 consultar al modelo de lenguaje.\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-06-13_at_12.45.10_PM.png?ref_type=heads' width='450' />\n",
        "\n",
        "**Importante:** Debido a las restricciones de esta API, lo ideal es utilizar la llave a la API de manera personal.\n",
        "\n",
        "\n",
        "Para mayor informaci칩n sobre **LangChain**, pueden revisar la documentaci칩n en el [presente enlace](https://python.langchain.com/v0.2/docs/tutorials/summarization/ )."
      ],
      "metadata": {
        "id": "dOIeEP9Ey_lF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CS_6MjRoWYMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install langchain_google_genai\n",
        "!pip install langchain-community\n",
        "!pip install langchain-experimental\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "LLbYWURudw2c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "82aJnnH0b0Oo"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDB95Ktc4MiqMcz8Z6xFx108_wpylgQTqo\" #AIzaSyDB95Ktc4MiqMcz8Z6xFx108_wpylgQTqo\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Carga y limpieza (0.5 puntos)**"
      ],
      "metadata": {
        "id": "kUgbzVtWUYq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para iniciar su titanica tarea de ense침arle a Joaqu칤n sobre las mejores peliculas del 칰ltimo tiempo, tiene que revisar los script de las siguientes 3 peliculas:\n",
        "* Dune 2\n",
        "* Under Paris\n",
        "* Joker\n",
        "\n",
        "Debe encontrar un patr칩n y obtener solamente el gui칩n de las pel칤culas. Para ello se recomienda utilizar m칠todos de b칰squeda y reemplazo que tienen los ``string`` en Python. Adicionalmente, puede usar filtros de expresiones regulares.\n",
        "\n",
        "Posterior a la limpieza de los guiones, debe considerar que el patr칩n se repite y es generalizable.\n"
      ],
      "metadata": {
        "id": "6I10Li9a7nez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scripts de peliculas\n",
        "dune2_script=\"https://scrapsfromtheloft.com/movies/dune-part-two-2024-transcript/\"\n",
        "underparis_script=\"https://scrapsfromtheloft.com/movies/under-paris-2024-transcript/\"\n",
        "joker_script=\"https://scrapsfromtheloft.com/movies/joker-2019-transcript/\""
      ],
      "metadata": {
        "id": "HpYuwfO_F0pD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "\n",
        "def load_website_data(url):\n",
        "\n",
        "  loader = WebBaseLoader(url)\n",
        "  website_data = loader.load()\n",
        "\n",
        "  return website_data[0].page_content\n",
        "\n",
        "def remove_text_before_marker(text):\n",
        "    match = re.search(\"\\t\\t\\n\\n\\n\\n \\n\\n\\n\\n\", text)\n",
        "    body = text[match.end():]\n",
        "\n",
        "    if \"* * *\" in body:\n",
        "        match = re.search(\"\\* \\* \\*\", body)\n",
        "\n",
        "        text_out = body[match.end():]\n",
        "    else:\n",
        "        text_out = body\n",
        "\n",
        "    match = re.search('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', text_out)\n",
        "    text_out = text_out[:match.start()]\n",
        "    return text_out"
      ],
      "metadata": {
        "id": "XUfvpxPD8v5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49c7a5c-6f61-4a69-ddb0-c6d38799867c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dune2 = load_website_data(dune2_script)\n",
        "underparis = load_website_data(underparis_script)\n",
        "joker = load_website_data(joker_script)"
      ],
      "metadata": {
        "id": "wXKJI15ky1Os"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dune2_text = remove_text_before_marker(dune2)\n",
        "underparis_text = remove_text_before_marker(underparis)\n",
        "joker_text = remove_text_before_marker(joker)"
      ],
      "metadata": {
        "id": "HeNVKEldy5fV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Aplicaci칩n (3.5 puntos)**\n",
        "\n",
        "Luego de limpiar los guiones, es posible generar la aplicaic칩n deseada con el LLM. Esta aplicaci칩n tiene que ser capaz de realizar las siguientes tareas.\n",
        "\n",
        "1. Utilizando una plantilla sobre el nombre del archivo o la URL, identifique el supuesto nombre de la pel칤cula.\n",
        "\n",
        "2. Genere un resumen en espa침ol de la pel칤cula y una nota evaluativa sobre la misma. El resumen debe tener entre 3 a 5 p치rrafos. Adem치s, obtener una evaluaci칩n de la pel칤cula con una calificaci칩n del 1 al 10, utilizando una LLM y el contexto entregado"
      ],
      "metadata": {
        "id": "7Btad-nZ9EyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.1 T칤tulo de la pel칤cula (0.5 puntos)**\n",
        "\n",
        "Para obtener el t칤tulo, utilic칠 la siguiente plantilla:\n",
        "```\n",
        " template = \"\"\"\n",
        "  What is the movie that appears in the description of this file or url?\n",
        "  You only give me the movie name, nothing more.\n",
        "  document/url: {script_path_url}\n",
        "  \"\"\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QcS80oN2-Gq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_title(script_path_url):\n",
        "  template = \"\"\"\n",
        "  What is the movie that appears in the description of this file or url?\n",
        "  You only give me the movie name, nothing more.\n",
        "  document/url: {script_path_url}\n",
        "  \"\"\"\n",
        "  prompt = PromptTemplate.from_template(template)\n",
        "  prompt_val = prompt.invoke({\"script_path_url\": script_path_url})\n",
        "  result = llm.invoke(prompt_val)\n",
        "\n",
        "  return result.content.strip()"
      ],
      "metadata": {
        "id": "yNIU3mmh-F5W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.2 Resumen (1.0 puntos)**\n",
        "\n",
        "Como se vi칩 en clases, las LLM no pueden manejar cadenas de texto muy largas, esto es debido a que, dependiendo de su naturaleza, solo manejan ventanas de contexto que estan asociadas a caracteristicas de la red y del entrenamiento utilizado.\n",
        "\n",
        "Por ello, es altamente importante que si se desea hacer un resumen del texto, este se haga realizando un tipo de map/reduce sobre el texto. De manera que en cada una de las iteraciones se vaya disminuyendo el tama침o del texto, pero hay que tener cuidado con que le modelo vaya guardando el contexto de escenas previas."
      ],
      "metadata": {
        "id": "muDXLfr0CabX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.chains import StuffDocumentsChain, LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "9sxX87HpDZiV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#No cambiar funci칩n\n",
        "\n",
        "def map_reduce_text(script, map_template, reduce_template):\n",
        "\n",
        "  # Map\n",
        "  \"\"\"\n",
        "  map_prompt, crear el prompt desde el template\n",
        "  map_chain, crear la cadena desde el prompt\n",
        "  \"\"\"\n",
        "  map_prompt = PromptTemplate.from_template(map_template)\n",
        "  map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "  # Reduce\n",
        "  \"\"\"\n",
        "  reduce_prompt, crear el prompt desde el template\n",
        "  reduce_chain, crear la cadena desde el prompt\n",
        "  \"\"\"\n",
        "  reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "  reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "  # Combine\n",
        "  \"\"\"\n",
        "  Combinar y reducir los documentos, utilizar StuffDocumentsChain\n",
        "  y ReduceDocuentsChain con un m치ximo de 4000 tokens\n",
        "  \"\"\"\n",
        "  combine_documents_chain = StuffDocumentsChain(\n",
        "      llm_chain=reduce_chain,\n",
        "      document_variable_name=\"document\",\n",
        "  )\n",
        "\n",
        "\n",
        "  reduce_documents_chain = ReduceDocumentsChain(\n",
        "      combine_documents_chain = combine_documents_chain,\n",
        "      collapse_documents_chain = combine_documents_chain,\n",
        "      token_max = 4000,\n",
        "  )\n",
        "\n",
        "  # Map/Reduce\n",
        "  \"\"\"\n",
        "  Uilizar MapReduceDocumentsChain\n",
        "  \"\"\"\n",
        "\n",
        "  map_reduce_chain = MapReduceDocumentsChain(\n",
        "    llm_chain=map_chain,\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    document_variable_name=\"document\",\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "  # Text splitter\n",
        "  \"\"\"\n",
        "  Usar RecursiveCharacterTextSplitter\n",
        "  \"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 15000,\n",
        "      chunk_overlap = 0\n",
        "  )\n",
        "\n",
        "  split_script = text_splitter.split_text(script)\n",
        "\n",
        "\n",
        "  # resultado\n",
        "  split_script = text_splitter.create_documents(split_script)\n",
        "\n",
        "  #result = map_reduce_chain.run()\n",
        "  #return result\n",
        "\n",
        "  result = map_reduce_chain.invoke(split_script)\n",
        "\n",
        "  return result[\"output_text\"]"
      ],
      "metadata": {
        "id": "jkxFMnCFDcgl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crear templates\n",
        "\n",
        "map_template_summary = \"\"\"\n",
        "Resume la siguiente parte del guion. Mant칠n los detalles importantes y el contexto:\n",
        "{document}\n",
        "\"\"\"\n",
        "\n",
        "reduce_template_summary = \"\"\"\n",
        "Combina los siguientes res칰menes en un solo resumen cohesivo. Aseg칰rate de mantener los detalles importantes y el contexto:\n",
        "{document}\n",
        "\"\"\"\n",
        "\n",
        "answer_summary = \"\"\"\n",
        "Genere un resumen en espa침ol del documento y una nota evaluativa sobre el mismo. El resumen debe tener entre 3 a 5 p치rrafos. Adem치s, obtener una evaluaci칩n de la pel칤cula con una calificaci칩n del 1 al 10.\n",
        "Este resumen debe ser creado SOLO a partir de esta versi칩n resumida del guion proporcionada a continuaci칩n. No debe incluir informaci칩n externa o conocimiento previo sobre la pel칤cula. Aqu칤 est치 el guion resumido:\n",
        "{document}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HsEJR0IGEZ8V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir resumenes de pel칤culas.\n",
        "dune2_reduce_summary = map_reduce_text(dune2_text, map_template_summary, reduce_template_summary)\n",
        "underparis_reduce_summary = map_reduce_text(underparis_text, map_template_summary, reduce_template_summary)\n",
        "joker_reduce_summary = map_reduce_text(joker_text, map_template_summary, reduce_template_summary)\n",
        "\n",
        "prompt = PromptTemplate.from_template(answer_summary)\n",
        "dune2_resumen = llm.invoke(prompt.invoke({\"document\": dune2_reduce_summary}))\n",
        "print(dune2_resumen.content)\n",
        "\n",
        "underparis_resumen = llm.invoke(prompt.invoke({\"document\": underparis_reduce_summary}))\n",
        "print(underparis_resumen.content)\n",
        "\n",
        "joker_resumen = llm.invoke(prompt.invoke({\"document\": joker_reduce_summary}))\n",
        "print(joker_resumen.content)"
      ],
      "metadata": {
        "id": "ijib42GaIFSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c63d8c-813c-4711-a8a0-3eba588cd7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Resumen de Dune\n",
            "\n",
            "\"Dune\" nos introduce en un universo de intrigas pol칤ticas y luchas por el poder, centrado en el planeta des칠rtico de Arrakis, hogar de la especia, una sustancia de gran valor que otorga longevidad y habilidades ps칤quicas. La historia sigue a Paul Atreides, un joven noble que se convierte en el salvador profetizado de los Fremen, un pueblo ind칤gena de Arrakis. \n",
            "\n",
            "Tras la destrucci칩n de la Casa Atreides a manos de los Harkonnen, sus enemigos, Paul y su madre, Jessica, se refugian entre los Fremen. Aunque inicialmente recelosos, los Fremen reconocen a Paul como el Lisan al-Gaib, un salvador profetizado, y a Jessica como la nueva Madre Superiora. Paul se integra a la cultura Fremen, aprendiendo sus costumbres y habilidades, gan치ndose el respeto de su l칤der, Stilgar. Se convierte en un Fedaykin, un guerrero Fremen, y recibe el nombre de Muad'Dib, \"El que se침ala el camino\".\n",
            "\n",
            "Mientras tanto, los Harkonnen, liderados por el Bar칩n Harkonnen y sus hijos, buscan destruir a los Fremen y controlar la producci칩n de especia. Paul, con la ayuda de los Fremen, se enfrenta a los Harkonnen, emergiendo como un l칤der carism치tico y poderoso. \n",
            "\n",
            "La historia se intensifica con el descubrimiento del arsenal at칩mico de los Atreides, que Paul podr칤a usar para vengarse de los Harkonnen. Sin embargo, 칠l busca una soluci칩n pac칤fica, aunque la presi칩n de los Fremen y los ataques continuos de los Harkonnen lo obligan a tomar las riendas de la lucha por la libertad. \n",
            "\n",
            "Finalmente, Paul y los Fremen se enfrentan a los Sardaukar, los soldados de 칠lite del Emperador. En una batalla 칠pica, Paul vence al Bar칩n Harkonnen y se revela como el Lisan al-Gaib. Su victoria amenaza el control del Emperador sobre Arrakis, provocando la preparaci칩n de las Grandes Casas para invadir el planeta. \n",
            "\n",
            "Paul, con su nuevo poder y la lealtad de los Fremen, amenaza con destruir los campos de especia si las Grandes Casas atacan, desencadenando una guerra santa que cambiar치 el destino de Arrakis y el universo conocido. La historia termina con Paul, ahora conocido como Muad'Dib, como el l칤der de los Fremen, listo para enfrentarse a las Grandes Casas y luchar por la libertad de su pueblo, dejando al p칰blico en suspenso ante el desenlace de esta 칠pica batalla por el control de Arrakis.\n",
            "\n",
            "\n",
            "## Evaluaci칩n\n",
            "\n",
            "La historia de \"Dune\", a pesar de su complejidad, se presenta de forma clara y atractiva. La trama, rica en intrigas pol칤ticas, conflictos morales y acci칩n, mantiene al lector interesado hasta el final. La construcci칩n de los personajes, especialmente Paul, es convincente, mostrando su evoluci칩n desde un joven noble a un l칤der carism치tico y poderoso. \n",
            "\n",
            "La exploraci칩n de temas como la profec칤a, la libertad, la responsabilidad y la lucha por el poder, junto a la descripci칩n detallada del mundo de Arrakis, enriquece la experiencia del lector.  Sin embargo, la historia se queda en un punto 치lgido, dejando al lector con la incertidumbre del desenlace de la batalla por Arrakis. \n",
            "\n",
            "**Calificaci칩n: 8/10** \n",
            "\n",
            "## Resumen de \"Lilith\"\n",
            "\n",
            "La historia comienza con un equipo de cient칤ficos marinos que estudian el \"s칠ptimo continente\", una enorme masa de basura en el Oc칠ano Pac칤fico Norte. Su investigaci칩n se centra en Lilith, un tibur칩n Mako anormalmente grande y agresivo. Durante su estudio, descubren un grupo de tiburones Mako hembras de tama침o inusual que cazan en manada. Chris, uno de los buzos, desaparece tras un encuentro con Lilith.\n",
            "\n",
            "Tres a침os despu칠s, en Par칤s, se descubre un proyectil de la Segunda Guerra Mundial en el Sena. Sophia Assalas, de la organizaci칩n SOS, sospecha que algo no est치 bien y descubre que Lilith est치 en el r칤o. Sophia cree que Lilith necesita ser rescatada, y convence al sargento Adil Faez de que la acompa침e a buscarla.\n",
            "\n",
            "Juntos encuentran a Lilith cerca de la Isla de la Ciudad. El equipo se prepara para rescatarla, pero Sophia les advierte del peligro que representa. Mientras tanto, Mika y Ben intentan atraer a Lilith de vuelta al oc칠ano usando una baliza reactivada. \n",
            "\n",
            "Adil y su equipo descubren que la se침al de la baliza ha sido interferida, y Sophia sospecha de Mika. Mika y su grupo se adentran en las catacumbas, donde se encuentran con un grupo de personas. \n",
            "\n",
            "Adil y su equipo llegan a las catacumbas y se encuentran con Mika y su grupo, pero Lilith aparece y se desata el caos. El equipo intenta evacuar las catacumbas, con muchas personas atrapadas y en peligro. \n",
            "\n",
            "La escena cambia a un hospital, donde Sophia est치 en shock tras la muerte de Leo. Caro y Markus informan a Adil sobre el descubrimiento de un nuevo tipo de tibur칩n en las catacumbas de Par칤s, llamado Lilith. Lilith es capaz de reproducirse por partenog칠nesis y se ha adaptado al agua dulce, lo que representa una amenaza para la ciudad.\n",
            "\n",
            "El prefecto ordena ocultar la informaci칩n, y Adil, Sophia y Adama planean infiltrarse en las catacumbas para destruir el nido de Lilith. Durante la operaci칩n, Poiccard es atacado y muere. El equipo logra escapar y se dirige a la superficie.\n",
            "\n",
            "Mientras tanto, la competici칩n de triatl칩n comienza, y Lilith ataca a los nadadores. Los soldados del ej칠rcito disparan a los tiburones, provocando una explosi칩n masiva.\n",
            "\n",
            "Adil, Sophia y Caro logran escapar de las catacumbas, y Sophia se queda atr치s para ayudar a Adil, que est치 herido. La escena termina con Sophia y Adil en el agua, rodeados de tiburones, dejando al espectador con la sensaci칩n de que la amenaza de los tiburones a칰n no ha terminado. \n",
            "\n",
            "## Nota Evaluativa\n",
            "\n",
            "El guion de \"Lilith\" presenta una historia llena de acci칩n y suspense, con una trama que combina elementos de ciencia ficci칩n, horror y thriller. La premisa de un tibur칩n gigante y adaptado al agua dulce que amenaza la ciudad de Par칤s es intrigante y crea una atm칩sfera de tensi칩n constante.\n",
            "\n",
            "La historia tiene un ritmo din치mico y mantiene al espectador en vilo con giros inesperados y momentos de alta intensidad. La relaci칩n entre Sophia y Adil, dos personajes con personalidades y motivaciones diferentes, aporta un toque de complejidad emocional a la trama.\n",
            "\n",
            "Sin embargo, el guion podr칤a beneficiarse de un desarrollo m치s profundo de algunos personajes y de una exploraci칩n m치s exhaustiva de las consecuencias sociales y pol칤ticas que conlleva la amenaza de Lilith. \n",
            "\n",
            "**Calificaci칩n: 7/10** \n",
            "\n",
            "## Resumen:\n",
            "\n",
            "El guion nos presenta a Arthur Fleck, un hombre con problemas mentales que lucha por sobrevivir en la ca칩tica Gotham City. La huelga de basureros ha sumido la ciudad en el caos, reflejando la propia lucha interna de Arthur. Reci칠n salido de un hospital psiqui치trico, Arthur busca un aumento en su medicaci칩n, pero su soledad y aislamiento lo hacen vulnerable a las presiones de la vida. \n",
            "\n",
            "Arthur se aferra a la esperanza de encontrar un espacio en la sociedad, anhelando la atenci칩n y la admiraci칩n de los dem치s. Su obsesi칩n con el comediante Murray Franklin y su fijaci칩n con Thomas Wayne, el jefe de su madre, Penny, reflejan su deseo de ser reconocido y su b칰squeda de una figura paternal.\n",
            "\n",
            "Sin embargo, su deseo de atenci칩n se convierte en un comportamiento err치tico y peligroso. La p칠rdida de su trabajo, tras ser descubierto llevando un arma al hospital, lo lleva a un punto de quiebre. El guion termina con Arthur, sin trabajo y desesperado por ser visto, dando un paso hacia la oscuridad y la violencia. \n",
            "\n",
            "## Evaluaci칩n:\n",
            "\n",
            "El guion presenta una premisa interesante, explorando la fragilidad mental de un individuo en un entorno social ca칩tico. La historia de Arthur, un hombre marginado que busca reconocimiento, es convincente y genera empat칤a. Sin embargo, el guion carece de detalles sobre la evoluci칩n de Arthur hacia la violencia, dejando al lector con preguntas sobre los motivadores detr치s de su comportamiento. \n",
            "\n",
            "La falta de profundidad en el desarrollo de los personajes secundarios, como Penny y Randall, debilita la construcci칩n del mundo y la conexi칩n emocional con Arthur. La historia se centra en la lucha interna de Arthur, pero la falta de contexto sobre su pasado y sus relaciones limita la comprensi칩n de sus acciones.\n",
            "\n",
            "**Calificaci칩n: 6/10**\n",
            "\n",
            "El guion presenta un potencial interesante, pero necesita un desarrollo m치s profundo de los personajes y la trama para alcanzar su m치ximo potencial. La falta de contexto y la falta de claridad en la evoluci칩n de Arthur hacia la violencia debilita la historia. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adicionalmente, Joaqu칤n sabe que su primo favorito le gusta ``Dune: Part 2`` por lo que le gustar칤a tener mayor informaci칩n al respecto, para ello realice las siguientes tareas:\n",
        "\n",
        "\n",
        "3. Genere un gr치fico que muestre los personajes de la pel칤cula con m치s apariciones en la misma.\n",
        "4. Genere una tabla en pandas con los 3 personajes que m치s aparecen, indicando el nombre del actor y su edad actual m치s uno (ojo edad + 1).\n",
        "5. Cree una funci칩n que responda preguntas sobre la pel칤cula bas치ndose en la informaci칩n del texto entregado (OJO: las preguntas y salidas deben ser en espa침ol). Luego, responda las siguientes preguntas:\n",
        "* 쯈u칠 y qui칠n es Lisan al-Gaib?\n",
        "* 쯈u칠 personaje no cree en la profec칤a pero es parte de ella?\n",
        "* 쮺u치l es el objetivo de Feyd-Rautha?\n",
        "6. Utilizando el top 3 de personajes que m치s aparecen en la pel칤cula, genere con el modelo LLM y utilizando el contexto del guion, las 6 estad칤sticas que demuestren las habilidades de los personajes: Intelligence, Strength, Charisma, Wisdom, Emotional Resilience, y Creativity."
      ],
      "metadata": {
        "id": "dD7YHYZSIWbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.3 Personajes (0.5 puntos)**\n",
        "\n",
        "En la siguiente secci칩n, tiene que entregar un template de personajes y redicci칩n"
      ],
      "metadata": {
        "id": "e_vdMMceJZBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_template_characters = \"\"\"\n",
        "Identifica los personajes en la siguiente parte del guion y cuenta sus apariciones. Incluye el nombre del personaje y el n칰mero de apariciones en este segmento:\n",
        "{document}\n",
        "\"\"\"\n",
        "\n",
        "reduce_template_characters = \"\"\"\n",
        "Combina las siguientes listas de personajes y sus apariciones en una sola lista cohesiva. Suma las apariciones de cada personaje:\n",
        "{document}\n",
        "\n",
        "El resultado debe tener el siguiente formato JSON:\n",
        "[\n",
        "  {{\"personaje\": \"Nombre del personaje\", \"apariciones\": N칰mero de apariciones}},\n",
        "  ...\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "answer_character_list = map_reduce_text(\n",
        "    dune2_text,\n",
        "    map_template_characters,\n",
        "    reduce_template_characters\n",
        ")"
      ],
      "metadata": {
        "id": "fIM5JVC5JWNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from itertools import count\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "\n",
        "def plot_characters(answer_character_list):\n",
        "  # Clear answer\n",
        "  answer_character_list = re.sub(r'\\s+', ' ', answer_character_list).strip()[8:-3]\n",
        "  character_pd = pd.DataFrame(json.loads(answer_character_list))\n",
        "\n",
        "  # distil the characters output\n",
        "  \"\"\"\n",
        "  Recomendaci칩n, utilizar un diccionario para ordenar los personajes\n",
        "  \"\"\"\n",
        "  # Ordenar personajes por aparici칩n\n",
        "  character_pd = character_pd.sort_values(by='apariciones', ascending=False)\n",
        "\n",
        "\n",
        "  # Create dataframe\n",
        "  \"\"\"\n",
        "  De diccionario a DataFrame\n",
        "  \"\"\"\n",
        "  Top3_characters = character_pd.head(3)\n",
        "\n",
        "  # Graficar datos\n",
        "  fig = px.bar(character_pd, x='personaje', y='apariciones', title='Apariciones de Personajes en la Pel칤cula')\n",
        "  fig.show()\n",
        "\n",
        "  #Retornar los personajes\n",
        "  return character_pd['personaje']\n"
      ],
      "metadata": {
        "id": "hJQ-RPYJKOKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista_personajes = plot_characters(answer_character_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Xx81ShvtyPhs",
        "outputId": "1658c9b7-b890-4327-ccf7-2b9ecdc650f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"2845352b-a0c7-4b0e-9c35-67cb0446cd5a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2845352b-a0c7-4b0e-9c35-67cb0446cd5a\")) {                    Plotly.newPlot(                        \"2845352b-a0c7-4b0e-9c35-67cb0446cd5a\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"personaje=%{x}\\u003cbr\\u003eapariciones=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"Stilgar\",\"Paul\",\"Harkonnen Soldiers (general)\",\"Jessica\",\"Chani\",\"Fremen (general)\",\"Gurney Halleck\",\"Fremen\",\"Harkonnen soldier\",\"Fedaykin fighter\",\"Rabban\",\"Feyd-Rautha Harkonnen\",\"Shishakli\",\"Man 1\",\"Baron Harkonnen\",\"Reverend Mother Mohiam\",\"Man (no identificado)\",\"Lady Margot Fenring\",\"Irulan\",\"Harkonnen Commander\",\"Gladiator arena announcer\",\"Sardaukar\",\"Bashar\",\"Alia\",\"Oldest Elder\",\"Usul\",\"Man 2\",\"Harkonnen Squad Leader\",\"Harkonnen Soldier 1\",\"Emperor\",\"Bene Gesserit sister 1\",\"Lanville\",\"Fremen Nuns\",\"Ancient Voice 2\",\"Ancient Voice 1\",\"Bene Gesserit sister 2\",\"Maker Keeper\",\"Jamis\",\"Watermasters\",\"Harkonnen Sniper\",\"Slave master\",\"Male Watermaster\",\"Sentinel Leader\",\"Old Watermaster\",\"Fremen Sentinel\",\"Harkonnen Lieutenant\",\"Guard\",\"Woman\",\"Man on Radio\",\"Man\",\"Commander\",\"Translator\",\"Elders\"],\"xaxis\":\"x\",\"y\":[48,44,34,33,31,26,23,16,15,12,11,10,10,7,7,6,6,6,5,5,4,4,4,4,4,4,4,3,3,3,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"personaje\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"apariciones\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Apariciones de Personajes en la Pel칤cula\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2845352b-a0c7-4b0e-9c35-67cb0446cd5a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.4 Actores principales (0.75 puntos)**\n",
        "\n",
        "Importante saber que el script **no** maneja informaci칩n de los actores, por ello, es importante que nuestra LLM tenga acceso a internet, de manera de poder realizar b칰squedas que nos ayuden a completar la informaci칩n consultada.\n",
        "\n",
        "Para esto, utilizaremos agentes combinados con react para realzar la consulta y asegurarnos de que la respuesta es correcta."
      ],
      "metadata": {
        "id": "2G2dx0XoLis4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import AgentType, initialize_agent"
      ],
      "metadata": {
        "id": "A9CHzsLDKPeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key para realizar una busqueda\n",
        "os.environ[\"SERPER_API_KEY\"] = 'd63e62662ef63eb9e44ab133d191f7a99a0024a3'"
      ],
      "metadata": {
        "id": "4ZSclMoFMGSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWQpu32y-UC8",
        "outputId": "934d3258-4ea2-49d8-a65f-902cc3aeb55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning:\n",
            "\n",
            "The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lista_personajes.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyQ_m8c9-cOg",
        "outputId": "03bfab4c-17c9-4932-b01d-8b96bb38fc2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3                         Stilgar\n",
              "1                            Paul\n",
              "6    Harkonnen Soldiers (general)\n",
              "Name: personaje, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "def get_actors_and_age(character):\n",
        "\n",
        "  # Inicializar tools y agente.\n",
        "  tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
        "  agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "  # Crear template de query\n",
        "  query_template_nombre = \"\"\"\n",
        "  Who plays {character} in Dune 2?. Answer must be only the actor name.\n",
        "  \"\"\"\n",
        "\n",
        "  query_template_edad = \"\"\"\n",
        "  \"What is the actor {actor_name} birth year?\"\n",
        "  \"\"\"\n",
        "\n",
        "  # Crear prompt y usar agente para la b칰squeda.\n",
        "  prompt_nombre = PromptTemplate.from_template(query_template_nombre)\n",
        "  prompt_edad = PromptTemplate.from_template(query_template_edad)\n",
        "\n",
        "  # Retornar Nombre y Edad + 1\n",
        "  nombre_actor = agent.invoke(prompt_nombre.invoke({\"character\": character}))['output']\n",
        "  edad_actor = agent.invoke(prompt_edad.invoke({\"actor_name\": nombre_actor}))['output']\n",
        "\n",
        "  try:\n",
        "    birth_year = int(edad_actor.strip())\n",
        "    current_year = datetime.datetime.now().year\n",
        "    actor_age_p1 = current_year - birth_year + 1\n",
        "  except ValueError:\n",
        "    actor_age_p1 = \"Edad no encontrada\"\n",
        "\n",
        "  return nombre_actor, actor_age_p1"
      ],
      "metadata": {
        "id": "sbGuaD6PMMA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nombre, edad = get_actors_and_age('Stilgar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKneceinAznV",
        "outputId": "a03932a9-8be5-4ec7-d0d0-4c4f0da9e3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find out who plays Stilgar in Dune 2. \n",
            "Action: google_serper\n",
            "Action Input: Who plays Stilgar in Dune 2?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mJavier Bardem\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the answer.\n",
            "Final Answer: Javier Bardem \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find the birth year of Javier Bardem. \n",
            "Action: google_serper\n",
            "Action Input: Javier Bardem birth year\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m1969\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: 1969 \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nombre, edad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU9P8ql9H9Ma",
        "outputId": "a7bf2772-16d9-4c62-acca-d1d1d99c5d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Javier Bardem 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tras distintas iteraciones de Prompts, descubrimos que preguntas con respuestas simples evitaban que google serper entrara en loop, por lo tanto decidimos separar la pregunta del nombre y la edad del actor por separado. Tambi칠n descubrimos que dado que es m치s sencillo obtener la fecha de nacimiento que la edad calculada, preguntamos por el a침o de nacimiento e hicimos la resta (m치s uno) para obtener la edad deseada."
      ],
      "metadata": {
        "id": "Kpf9H61qMxal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.5 Personajes Stats (0.5 puntos)**\n",
        "\n",
        "Esta parte es similar al punto 2. La clave esta en crear un buen prompting que nos permita generar las estad칤sticas basandonos en una b칰squeda por map/reduce.\n",
        "\n",
        "Tras la b칰squeda, la idea es tener una funci칩n de Python que nos permita generar el gr치fico deseado y tener el resumen de los personajes.\n"
      ],
      "metadata": {
        "id": "MtQqA40sM09E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def map_reduce_text(script, character):\n",
        "  # Map\n",
        "  map_template = \"\"\"\n",
        "  You are analyzing a movie script. Extract and analyze the following attributes for the character {character}:\n",
        "  Intelligence, Charisma, Strength, Wisdom, Emotional Resilience, and Creativity.\n",
        "  Provide your analysis in a clear and detailed manner.\n",
        "  \"\"\"\n",
        "\n",
        "  # crear prompt y cadena\n",
        "  #map_template += template_complemt\n",
        "  map_prompt = PromptTemplate.from_template(map_template)\n",
        "  map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "  # Reduce\n",
        "  reduce_template = \"\"\"\n",
        "  Based on the provided analysis, rate the character {character} on a scale from 1 to 10 for the following attributes:\n",
        "  Intelligence, Charisma, Strength, Wisdom, Emotional Resilience, and Creativity.\n",
        "  Provide a summary for each attribute explaining the rating in spanish.\n",
        "  Besides the summary add a json format text:\n",
        "  [\n",
        "    {{\"Intelligence\": Intelligence Rate, \"Charisma\": Charisma rate, ...}},\n",
        "    ...\n",
        "  ]\n",
        "\n",
        "  Split the summary from the json text with the text ***\n",
        "  \"\"\"\n",
        "  reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "  reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "  # Reduce\n",
        "  \"\"\"\n",
        "  Reducir y combinar los documentos con un m치ximo de 4000 tokens\n",
        "  \"\"\"\n",
        "  combine_documents_chain = StuffDocumentsChain(\n",
        "      llm_chain=reduce_chain,\n",
        "      document_variable_name=\"character\",\n",
        "  )\n",
        "\n",
        "  reduce_documents_chain = ReduceDocumentsChain(\n",
        "      combine_documents_chain = combine_documents_chain,\n",
        "      collapse_documents_chain = combine_documents_chain,\n",
        "      token_max = 4000,\n",
        "  )\n",
        "\n",
        "  # Map/Reduce\n",
        "  \"\"\"\n",
        "  Uilizar MapReduceDocumentsChain\n",
        "  \"\"\"\n",
        "\n",
        "  map_reduce_chain = MapReduceDocumentsChain(\n",
        "    llm_chain=map_chain,\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    document_variable_name=\"character\",\n",
        "    return_intermediate_steps=False,\n",
        "  )\n",
        "\n",
        "\n",
        "  # Text splitter\n",
        "  \"\"\"\n",
        "  Usar RecursiveCharacterTextSplitter\n",
        "  \"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 15000,\n",
        "      chunk_overlap = 0\n",
        "  )\n",
        "\n",
        "  split_script = text_splitter.split_text(script)\n",
        "  split_script = text_splitter.create_documents(split_script)\n",
        "\n",
        "  # resultado\n",
        "  result = map_reduce_chain.invoke(split_script)\n",
        "  return result[\"output_text\"]\n",
        "\n",
        "\n",
        "# Formato del perfil\n",
        "def format_profile(answer_character_profile):\n",
        "  \"\"\"\n",
        "  Crear un json con las caracteristicas y que retorne\n",
        "  (final_profile, stats) del personaje\n",
        "  \"\"\"\n",
        "  match = re.search(\"\\*\\*\\*\", answer_character_profile)\n",
        "  json_text = answer_character_profile[match.end():].strip()\n",
        "  json_text = re.sub(r'\\s+', ' ', answer_character_list[match.end():].strip()).strip()\n",
        "  json_text = re.sub(r'\\`', ' ', json_text).strip()\n",
        "  json_text = re.sub(r'json', ' ', json_text).strip()\n",
        "\n",
        "  stats = json.loads(json_text)\n",
        "  final_profile = answer_character_profile[:match.start()]\n",
        "  return (final_profile, stats[0])\n"
      ],
      "metadata": {
        "id": "ha1zVrtkNaF-"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_character_list = map_reduce_text(\n",
        "    dune2_text,\n",
        "    'Paul Atreides'\n",
        ")"
      ],
      "metadata": {
        "id": "U5jl1RA_ZhzJ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_profile, stats = format_profile(answer_character_list)"
      ],
      "metadata": {
        "id": "0oduYAOZPaL_"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_profile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jzu-Qd_ts0j",
        "outputId": "99bc7310-5adb-4db5-a5bd-8825a15814db"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Character Attribute Analysis: Paul Atreides\n",
            "\n",
            "Here's a breakdown of Paul's attributes on a scale of 1 to 10, with explanations for each rating:\n",
            "\n",
            "**Intelligence:** 9\n",
            "\n",
            "Paul demonstrates a remarkable understanding of complex political machinations, strategic planning, and cultural nuances. He quickly grasps the importance of spice control, the power dynamics of the Great Houses, and the Fremen culture. His ability to anticipate threats, manipulate others, and devise effective strategies demonstrates a sharp and strategic mind. \n",
            "\n",
            "**Charisma:** 8\n",
            "\n",
            "Paul possesses a natural charisma that attracts followers and inspires loyalty. He commands respect through his confident leadership, bold declarations, and unwavering resolve. He uses his charm and manipulation skills to influence others, gaining their trust and support.\n",
            "\n",
            "**Strength:** 7\n",
            "\n",
            "Paul's physical prowess is demonstrated through his fighting skills, evident in his duel with Feyd-Rautha. However, his strength also lies in his mental fortitude and unwavering willpower. He remains composed under immense pressure, enduring loss and betrayal with remarkable resilience.\n",
            "\n",
            "**Wisdom:** 8\n",
            "\n",
            "Paul demonstrates a deep understanding of power, strategic foresight, and the importance of controlling resources. He strategically leverages his knowledge of Fremen culture and customs to his advantage. He also recognizes the potential consequences of his actions and plans accordingly. \n",
            "\n",
            "**Emotional Resilience:** 8\n",
            "\n",
            "Paul faces immense challenges, including loss, betrayal, and the burden of his visions. He channels his grief into a powerful drive for revenge and a commitment to his new people. Despite facing overwhelming odds, he remains determined and focused on his goals, showcasing remarkable resilience.\n",
            "\n",
            "**Creativity:** 7\n",
            "\n",
            "Paul demonstrates creativity through his innovative strategies, combining traditional Fremen tactics with his own unique approaches. He adapts to changing circumstances, learning from his mistakes and finding new ways to achieve his objectives. His ability to think outside the box and devise effective solutions showcases his creative thinking.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci칩n para gr치ficar stats. No Tocar.\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "def plot_stats(stats, character_name=\"Paul Atreides\"):\n",
        "    base_stats = [\n",
        "        \"Intelligence\", \"Charisma\", \"Strength\",\n",
        "        \"Wisdom\", \"Emotional Resilience\", \"Creativity\"\n",
        "    ]\n",
        "    for stat in base_stats:\n",
        "        if stat not in stats:\n",
        "            stats[stat] = 0\n",
        "\n",
        "    labels = list(stats.keys())\n",
        "    stats_values = list(stats.values())\n",
        "    stats_values += stats_values[:1]\n",
        "    labels += labels[:1]\n",
        "\n",
        "    # Plotly figure\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=stats_values,\n",
        "        theta=labels,\n",
        "        fill='toself',\n",
        "        name=character_name\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        polar=dict(\n",
        "            radialaxis=dict(\n",
        "                visible=True,\n",
        "                range=[0, max(stats_values)]\n",
        "            )\n",
        "        ),\n",
        "        showlegend=False,\n",
        "        title=character_name\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "fig = plot_stats(stats)"
      ],
      "metadata": {
        "id": "IXAHPyRSP6HY"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "f88RQHTlhCJa",
        "outputId": "81289fa7-37e9-4d2b-de22-b9f4a24eca80"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"6027cc73-e697-4691-9b1e-9f22cbd06ebd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6027cc73-e697-4691-9b1e-9f22cbd06ebd\")) {                    Plotly.newPlot(                        \"6027cc73-e697-4691-9b1e-9f22cbd06ebd\",                        [{\"fill\":\"toself\",\"name\":\"Paul Atreides\",\"r\":[9,8,7,8,8,7,9],\"theta\":[\"Intelligence\",\"Charisma\",\"Strength\",\"Wisdom\",\"Emotional Resilience\",\"Creativity\",\"Intelligence\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"radialaxis\":{\"visible\":true,\"range\":[0,9]}},\"showlegend\":false,\"title\":{\"text\":\"Paul Atreides\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6027cc73-e697-4691-9b1e-9f22cbd06ebd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Comentar (0.25 puntos)**\n",
        "\n",
        "En primer lugar, se utiliza un prompt para resumir el script, se especifica un enfoque particular para resolver la pregunta en cuesti칩n.\n",
        "\n",
        "Para la soluci칩n del problema utilizamos un prompt que indicaba especificamente una serie de caracteres para identificar el resumen del json. Facilitando la extracci칩n respectiva.\n",
        "\n",
        "1. 쯈u칠 otras tareas se podr칤a realizar? De dos ejemplos con la metodolog칤a asociada.\n",
        "\n",
        "Dado que la respuesta del modelo es Resumen *** json. Se puede utilizar para cualquier problema que requiera un conjunto de datos estandarizados.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "\n",
        "*   Analisis de g칠nero de la pelicula: Se podr칤a usar un prompt que analice los distintos g칠neros de la pelicula y devuelta un json con un puntaje de proporci칩n de cada genero. (Una pelicula con genero romantica puede tener romance, pero no ser especificamente de romance)\n",
        "*   Lineas de participaci칩n de personajes: Similar a unos de los problemas anteriores, se puede hacer un conteo de la participaci칩n, a nivel de lineas en el script, para cada personaje.\n",
        "\n",
        "\n",
        "2. 쮺ual es la importancia de los prompt y como estos afectan al desempe침o de los LLM?\n",
        "\n",
        "Los prompts gui치n al modelo para genear respuestas espec칤ficas. La idea es tener un prompt claro para reducir ambuguedades y mejorar precisi칩n.\n",
        "\n",
        "3. 쮸lguna de sus respuestas fue una 'alucinaci칩n'? 쯇or qu칠 sucede esto?\n",
        "\n",
        "Cuando utilizamos el modelo para generar un resumen del script de Dune parte 2 el modelo utiliz칩 la informaci칩n que conoc칤a previamente, ya que el resumen generado para Dune parte 2 correspondia a un resumen de Dune parte 1.\n",
        "\n",
        "Seg칰n lo investigado (chatGPT), esto pudo ocurrir porque el modelo debe haber prioirizado informaci칩n ya conocida sobre Dune parte 1 en lugar de enforcarse en el nuevo gui칩n proporcionado.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_APhHBPXQXTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Reinforcement Learning (2.0 puntos)**\n",
        "\n",
        "En esta secci칩n van a usar m칠todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
      ],
      "metadata": {
        "id": "0hmHHQ9BuyAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq gymnasium stable_baselines3\n",
        "!pip install -qqq swig\n",
        "!pip install -qqq gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOcejYb6uzOO",
        "outputId": "3f20889e-0861-4d6a-f611-69b101993c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Blackjack (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Joaqu칤n es fan치tico del Blackjack, por lo que en esta subsecci칩n implementar치n m칠todos de RL y as칤 generar una estrategia para que pueda ~~ir al casino a  hacerse millonario~~ aprender a resolver problemas mediante RL.\n",
        "\n",
        "Comencemos primero preparando el ambiente. El siguiente bloque de c칩digo transforma las observaciones del ambiente a `np.array`:\n"
      ],
      "metadata": {
        "id": "qBPet_Mq8dX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)"
      ],
      "metadata": {
        "id": "LpZ8bBKk9ZlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.1 Descripci칩n de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripci칩n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
      ],
      "metadata": {
        "id": "ZJ6J1_-Y9nHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El ambiente de Blackjack en Gymnasium modela el juego de cartas Blackjack usando un Proceso de Decisi칩n de Markov (MDP).\n",
        "\n",
        "묖tEstados: Consisten en una tupla de tres elementos: la suma actual del jugador, el valor de la carta visible del crupier (1-10), y si el jugador tiene un as utilizable (0 o 1).\n",
        "\n",
        "묖tAcciones: Discretas, con dos posibles: pedir carta (hit) o plantarse (stick).\n",
        "\n",
        "묖tRecompensas: +1 por ganar, -1 por perder, 0 por empatar y +1.5 por ganar con un Blackjack natural."
      ],
      "metadata": {
        "id": "G5i1Wt1p770x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "* Simule un escenario en donde se escojan acciones aleatorias. Repita esta\n",
        "simulaci칩n 5000 veces y reporte el promedio y desviaci칩n de las recompensas.\n",
        "* 쮺칩mo calificar칤a el performance de esta pol칤tica?\n",
        "* 쮺칩mo podr칤a interpretar las recompensas obtenidas?"
      ],
      "metadata": {
        "id": "pmcX6bRC9agQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "# Simular 5000 episodios con acciones aleatorias\n",
        "n_episodes = 5000\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    observation, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Acci칩n aleatoria\n",
        "        observation, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviaci칩n est치ndar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "id": "RHCfKN7NGi1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e169de94-6550-4da2-d9cd-4dd70b8287e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -0.387\n",
            "Desviaci칩n est치ndar de las recompensas: 0.9017932135473187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. El performance de la pol칤tica aleatoria es bajo. El promedio de recompensas de -0.387 indica que, en promedio, la pol칤tica aleatoria tiende a perder m치s juegos de los que gana.\n",
        "\n",
        "2. Desviaci칩n Est치ndar (0.9018): Una desviaci칩n est치ndar relativamente alta muestra que los resultados de los episodios son variables, lo que es esperado con una pol칤tica aleatoria, ya que las decisiones no siguen una estrategia coherente."
      ],
      "metadata": {
        "id": "nuKbcR2RCbVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
      ],
      "metadata": {
        "id": "LEO_dY4x_SJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete([32, 11, 2])\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Crear el modelo A2C\n",
        "model = A2C(\"MlpPolicy\", env, verbose=1, seed=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=50000)\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"a2c_blackjack\")\n",
        "\n",
        "# Cargar el modelo\n",
        "model = A2C.load(\"a2c_blackjack\")"
      ],
      "metadata": {
        "id": "T0sp8XWsGg4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c149f8c-47f5-4b3d-9b7e-48ff65295a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 212      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.137   |\n",
            "|    explained_variance | 0.24     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 0.00521  |\n",
            "|    value_loss         | 0.737    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 278      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.144   |\n",
            "|    explained_variance | 0.741    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -0.00609 |\n",
            "|    value_loss         | 0.105    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 311      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0459  |\n",
            "|    explained_variance | -0.264   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 0.00605  |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 332      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0555  |\n",
            "|    explained_variance | 0.214    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 0.00258  |\n",
            "|    value_loss         | 0.909    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 346      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.208   |\n",
            "|    explained_variance | 0.0625   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 0.00687  |\n",
            "|    value_loss         | 0.774    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 355      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.206   |\n",
            "|    explained_variance | -0.0357  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -0.77    |\n",
            "|    value_loss         | 0.852    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 362      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.344    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -0.0117  |\n",
            "|    value_loss         | 0.641    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 367      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.251   |\n",
            "|    explained_variance | 0.308    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -0.0515  |\n",
            "|    value_loss         | 0.912    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 365      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.615   |\n",
            "|    explained_variance | -1.12    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -0.233   |\n",
            "|    value_loss         | 0.332    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 355      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | -0.232   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 0.0641   |\n",
            "|    value_loss         | 0.786    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 354      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.197   |\n",
            "|    explained_variance | 0.101    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -0.0813  |\n",
            "|    value_loss         | 0.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 359      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.114   |\n",
            "|    explained_variance | 0.851    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -0.0105  |\n",
            "|    value_loss         | 0.204    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 361      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | -0.231   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -0.25    |\n",
            "|    value_loss         | 1.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 365      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.169   |\n",
            "|    explained_variance | 0.626    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -0.00686 |\n",
            "|    value_loss         | 0.132    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 368      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0886  |\n",
            "|    explained_variance | -0.117   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 0.044    |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 371      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.225   |\n",
            "|    explained_variance | -0.48    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 1.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.15    |\n",
            "|    explained_variance | 0.156    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 0.699    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0983  |\n",
            "|    explained_variance | 0.463    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -0.0139  |\n",
            "|    value_loss         | 0.393    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.19     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 0.0689   |\n",
            "|    value_loss         | 0.461    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 372      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | -2.39    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 0.0571   |\n",
            "|    value_loss         | 0.967    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 368      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | -254     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | -0.174   |\n",
            "|    value_loss         | 0.279    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 370       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.125    |\n",
            "|    explained_variance | -3.68e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | -0.132    |\n",
            "|    value_loss         | 0.461     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 372      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.106   |\n",
            "|    explained_variance | 0.247    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | -0.0359  |\n",
            "|    value_loss         | 0.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0564  |\n",
            "|    explained_variance | 0.00232  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 0.0209   |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0519  |\n",
            "|    explained_variance | -0.416   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -0.00129 |\n",
            "|    value_loss         | 0.288    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.162   |\n",
            "|    explained_variance | 0.671    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -0.0485  |\n",
            "|    value_loss         | 0.398    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 378       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0101   |\n",
            "|    explained_variance | 0.603     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -0.000861 |\n",
            "|    value_loss         | 0.237     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.839    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -0.0878  |\n",
            "|    value_loss         | 0.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 38       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.181   |\n",
            "|    explained_variance | 0.645    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -0.0419  |\n",
            "|    value_loss         | 0.342    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0467  |\n",
            "|    explained_variance | -1.31    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -0.00783 |\n",
            "|    value_loss         | 1.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 41       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.363    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 0.527    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00473 |\n",
            "|    explained_variance | 0.276    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 0.00033  |\n",
            "|    value_loss         | 0.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 43       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0191  |\n",
            "|    explained_variance | 0.752    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 0.000846 |\n",
            "|    value_loss         | 0.148    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.000985 |\n",
            "|    explained_variance | 0.79      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | -4.16e-05 |\n",
            "|    value_loss         | 0.208     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 46       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0447  |\n",
            "|    explained_variance | -0.0418  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 0.014    |\n",
            "|    value_loss         | 0.667    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 47       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.043   |\n",
            "|    explained_variance | 0.36     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | -0.00326 |\n",
            "|    value_loss         | 0.666    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 381      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 48       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0153  |\n",
            "|    explained_variance | -39.1    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | -0.00103 |\n",
            "|    value_loss         | 0.544    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 382      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 49       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.126   |\n",
            "|    explained_variance | -1.34    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 0.0263   |\n",
            "|    value_loss         | 0.822    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 383       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.12     |\n",
            "|    explained_variance | 0.168     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -0.000233 |\n",
            "|    value_loss         | 0.554     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 383      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 52       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0548  |\n",
            "|    explained_variance | 0.593    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -0.00835 |\n",
            "|    value_loss         | 0.282    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 53       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.111   |\n",
            "|    explained_variance | 0.252    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | -0.304   |\n",
            "|    value_loss         | 0.484    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0833  |\n",
            "|    explained_variance | 0.662    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | -0.032   |\n",
            "|    value_loss         | 0.792    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 56       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0774  |\n",
            "|    explained_variance | -11.4    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -0.0164  |\n",
            "|    value_loss         | 0.452    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 381      |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 57       |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.13    |\n",
            "|    explained_variance | -0.265   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | -0.0316  |\n",
            "|    value_loss         | 1.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 382      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 58       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.147   |\n",
            "|    explained_variance | 0.243    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | -0.125   |\n",
            "|    value_loss         | 0.511    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 383      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 59       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0413  |\n",
            "|    explained_variance | 0.0998   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 0.00254  |\n",
            "|    value_loss         | 0.597    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 384      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 61       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.311    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 0.661    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 385      |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 62       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.164   |\n",
            "|    explained_variance | -0.173   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | -0.0681  |\n",
            "|    value_loss         | 1.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 385      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 63       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0326  |\n",
            "|    explained_variance | 0.259    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 0.00472  |\n",
            "|    value_loss         | 0.415    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 385      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 64       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0631  |\n",
            "|    explained_variance | 0.675    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -0.00754 |\n",
            "|    value_loss         | 0.451    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 382       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 66        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0364   |\n",
            "|    explained_variance | -3.78e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -0.00457  |\n",
            "|    value_loss         | 0.579     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 68        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0135   |\n",
            "|    explained_variance | 0.662     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -0.000894 |\n",
            "|    value_loss         | 0.324     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 70       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0519  |\n",
            "|    explained_variance | 0.703    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -0.00692 |\n",
            "|    value_loss         | 0.294    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 375       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 71        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0294   |\n",
            "|    explained_variance | 0.546     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -0.000736 |\n",
            "|    value_loss         | 0.524     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 73       |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0574  |\n",
            "|    explained_variance | 0.919    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 0.00115  |\n",
            "|    value_loss         | 0.0851   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 74       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.12     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -0.0171  |\n",
            "|    value_loss         | 0.531    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 75        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0055   |\n",
            "|    explained_variance | 0.792     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -0.000402 |\n",
            "|    value_loss         | 0.249     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 76       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0424  |\n",
            "|    explained_variance | -0.437   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -0.00865 |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 78       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0102  |\n",
            "|    explained_variance | 0.567    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | 0.00207  |\n",
            "|    value_loss         | 0.278    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 79       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0283  |\n",
            "|    explained_variance | 0.64     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | -0.245   |\n",
            "|    value_loss         | 0.236    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 374       |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 81        |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00106  |\n",
            "|    explained_variance | -1.66e+04 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 0.000144  |\n",
            "|    value_loss         | 1         |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 372      |\n",
            "|    iterations         | 6200     |\n",
            "|    time_elapsed       | 83       |\n",
            "|    total_timesteps    | 31000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.156   |\n",
            "|    explained_variance | 0.357    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6199     |\n",
            "|    policy_loss        | 0.0803   |\n",
            "|    value_loss         | 0.888    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 372      |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 84       |\n",
            "|    total_timesteps    | 31500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0416  |\n",
            "|    explained_variance | 0.696    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | -0.00392 |\n",
            "|    value_loss         | 0.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 373      |\n",
            "|    iterations         | 6400     |\n",
            "|    time_elapsed       | 85       |\n",
            "|    total_timesteps    | 32000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0693  |\n",
            "|    explained_variance | 0.0997   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6399     |\n",
            "|    policy_loss        | 0.00103  |\n",
            "|    value_loss         | 1.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 374      |\n",
            "|    iterations         | 6500     |\n",
            "|    time_elapsed       | 86       |\n",
            "|    total_timesteps    | 32500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0783  |\n",
            "|    explained_variance | 0.347    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6499     |\n",
            "|    policy_loss        | 0.00109  |\n",
            "|    value_loss         | 0.657    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 6600     |\n",
            "|    time_elapsed       | 87       |\n",
            "|    total_timesteps    | 33000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0518  |\n",
            "|    explained_variance | 0.129    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6599     |\n",
            "|    policy_loss        | 0.00971  |\n",
            "|    value_loss         | 0.965    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 6700     |\n",
            "|    time_elapsed       | 89       |\n",
            "|    total_timesteps    | 33500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.124   |\n",
            "|    explained_variance | -0.24    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6699     |\n",
            "|    policy_loss        | 0.0632   |\n",
            "|    value_loss         | 0.413    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 6800     |\n",
            "|    time_elapsed       | 90       |\n",
            "|    total_timesteps    | 34000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.193   |\n",
            "|    explained_variance | 0.385    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6799     |\n",
            "|    policy_loss        | -0.00822 |\n",
            "|    value_loss         | 0.117    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 91        |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00525  |\n",
            "|    explained_variance | 0.225     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | -0.000583 |\n",
            "|    value_loss         | 0.837     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 7000      |\n",
            "|    time_elapsed       | 92        |\n",
            "|    total_timesteps    | 35000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00324  |\n",
            "|    explained_variance | -2.11e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6999      |\n",
            "|    policy_loss        | 0.000598  |\n",
            "|    value_loss         | 1.45      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 376       |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 94        |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0229   |\n",
            "|    explained_variance | -3.49e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 0.00814   |\n",
            "|    value_loss         | 1.24      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 7200     |\n",
            "|    time_elapsed       | 95       |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0653  |\n",
            "|    explained_variance | 0.199    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | -0.00747 |\n",
            "|    value_loss         | 0.985    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 375      |\n",
            "|    iterations         | 7300     |\n",
            "|    time_elapsed       | 97       |\n",
            "|    total_timesteps    | 36500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.138   |\n",
            "|    explained_variance | 0.319    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7299     |\n",
            "|    policy_loss        | -0.0646  |\n",
            "|    value_loss         | 1.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 7400     |\n",
            "|    time_elapsed       | 98       |\n",
            "|    total_timesteps    | 37000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0162  |\n",
            "|    explained_variance | 0.592    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | 0.00047  |\n",
            "|    value_loss         | 0.371    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 99       |\n",
            "|    total_timesteps    | 37500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00165 |\n",
            "|    explained_variance | 0.205    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | 0.000112 |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 377       |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00238  |\n",
            "|    explained_variance | 0.619     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | -0.000135 |\n",
            "|    value_loss         | 0.213     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 7700     |\n",
            "|    time_elapsed       | 101      |\n",
            "|    total_timesteps    | 38500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00281 |\n",
            "|    explained_variance | 0.409    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7699     |\n",
            "|    policy_loss        | 0.000181 |\n",
            "|    value_loss         | 0.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 7800     |\n",
            "|    time_elapsed       | 102      |\n",
            "|    total_timesteps    | 39000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0321  |\n",
            "|    explained_variance | 0.778    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7799     |\n",
            "|    policy_loss        | -0.00475 |\n",
            "|    value_loss         | 0.213    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 104      |\n",
            "|    total_timesteps    | 39500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.134   |\n",
            "|    explained_variance | -0.103   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | 0.121    |\n",
            "|    value_loss         | 0.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 8000     |\n",
            "|    time_elapsed       | 105      |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0566  |\n",
            "|    explained_variance | 0.668    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | -0.00425 |\n",
            "|    value_loss         | 0.243    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 107      |\n",
            "|    total_timesteps    | 40500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00171 |\n",
            "|    explained_variance | -0.141   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | 0.000256 |\n",
            "|    value_loss         | 0.909    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 376      |\n",
            "|    iterations         | 8200     |\n",
            "|    time_elapsed       | 108      |\n",
            "|    total_timesteps    | 41000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0425  |\n",
            "|    explained_variance | 0.281    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8199     |\n",
            "|    policy_loss        | 0.0126   |\n",
            "|    value_loss         | 0.879    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 376       |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 110       |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.000343 |\n",
            "|    explained_variance | -3.72     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 2.78e-05  |\n",
            "|    value_loss         | 1.32      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 8400     |\n",
            "|    time_elapsed       | 111      |\n",
            "|    total_timesteps    | 42000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00352 |\n",
            "|    explained_variance | -0.211   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8399     |\n",
            "|    policy_loss        | 0.000606 |\n",
            "|    value_loss         | 0.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 377      |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 112      |\n",
            "|    total_timesteps    | 42500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0963  |\n",
            "|    explained_variance | 0.167    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | -0.0117  |\n",
            "|    value_loss         | 0.208    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 8600     |\n",
            "|    time_elapsed       | 113      |\n",
            "|    total_timesteps    | 43000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.106   |\n",
            "|    explained_variance | 0.604    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8599     |\n",
            "|    policy_loss        | -0.0181  |\n",
            "|    value_loss         | 0.216    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 8700     |\n",
            "|    time_elapsed       | 114      |\n",
            "|    total_timesteps    | 43500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0821  |\n",
            "|    explained_variance | 0.396    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8699     |\n",
            "|    policy_loss        | 0.0273   |\n",
            "|    value_loss         | 0.499    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 8800     |\n",
            "|    time_elapsed       | 116      |\n",
            "|    total_timesteps    | 44000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.071   |\n",
            "|    explained_variance | 0.145    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8799     |\n",
            "|    policy_loss        | -0.00553 |\n",
            "|    value_loss         | 0.839    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 8900     |\n",
            "|    time_elapsed       | 117      |\n",
            "|    total_timesteps    | 44500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00012 |\n",
            "|    explained_variance | 0.029    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8899     |\n",
            "|    policy_loss        | 1.13e-05 |\n",
            "|    value_loss         | 0.551    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9000     |\n",
            "|    time_elapsed       | 118      |\n",
            "|    total_timesteps    | 45000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.102   |\n",
            "|    explained_variance | 0.269    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8999     |\n",
            "|    policy_loss        | 0.432    |\n",
            "|    value_loss         | 0.797    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9100     |\n",
            "|    time_elapsed       | 119      |\n",
            "|    total_timesteps    | 45500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0502  |\n",
            "|    explained_variance | 0.158    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9099     |\n",
            "|    policy_loss        | 0.0174   |\n",
            "|    value_loss         | 0.545    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 379       |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 121       |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0004   |\n",
            "|    explained_variance | 0.675     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | -2.54e-05 |\n",
            "|    value_loss         | 0.278     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 378      |\n",
            "|    iterations         | 9300     |\n",
            "|    time_elapsed       | 122      |\n",
            "|    total_timesteps    | 46500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0181  |\n",
            "|    explained_variance | 0.283    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9299     |\n",
            "|    policy_loss        | -0.00438 |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 378       |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 124       |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.000175 |\n",
            "|    explained_variance | 0.363     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 1.3e-05   |\n",
            "|    value_loss         | 0.446     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 379      |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 125      |\n",
            "|    total_timesteps    | 47500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0822  |\n",
            "|    explained_variance | -0.099   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | -0.00546 |\n",
            "|    value_loss         | 0.267    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 379       |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 126       |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0248   |\n",
            "|    explained_variance | -2.22e+03 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | -0.721    |\n",
            "|    value_loss         | 0.605     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 127      |\n",
            "|    total_timesteps    | 48500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.05    |\n",
            "|    explained_variance | 0.874    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | -0.00564 |\n",
            "|    value_loss         | 0.221    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 128      |\n",
            "|    total_timesteps    | 49000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0422  |\n",
            "|    explained_variance | 0.0733   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 0.0111   |\n",
            "|    value_loss         | 0.926    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 380      |\n",
            "|    iterations         | 9900     |\n",
            "|    time_elapsed       | 129      |\n",
            "|    total_timesteps    | 49500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.147   |\n",
            "|    explained_variance | -5.27    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9899     |\n",
            "|    policy_loss        | -0.0345  |\n",
            "|    value_loss         | 0.675    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 381       |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 131       |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0113   |\n",
            "|    explained_variance | 0.21      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | -0.000889 |\n",
            "|    value_loss         | 0.641     |\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
        "\n",
        "* Repita el ejercicio 2.1.2 pero utilizando el modelo entrenado.\n",
        "* 쮺칩mo es el performance de su agente?\n",
        "* 쮼s mejor o peor que el escenario baseline?"
      ],
      "metadata": {
        "id": "E-bpdb8wZID1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete([32, 11, 2])\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = A2C.load(\"a2c_blackjack\")\n",
        "\n",
        "# Simular 5000 episodios con el modelo entrenado\n",
        "n_episodes = 5000\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward[0]\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviaci칩n est치ndar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "id": "S7jdmnTwGePD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2537123f-47b2-4edd-f0de-463009013cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -0.0712\n",
            "Desviaci칩n est치ndar de las recompensas: 0.9466417273710259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparado con el escenario baseline, donde el promedio de recompensas era -0.387, el agente entrenado muestra una mejora, ya que la p칠rdida promedio es menor. Esto sugiere que el agente ha aprendido a tomar decisiones m치s efectivas que una pol칤tica aleatoria."
      ],
      "metadata": {
        "id": "IUXYAG_jGWTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.5 Estudio de acciones (0.2 puntos)**\n",
        "\n",
        "* Genere una funci칩n que reciba un estado y retorne la accion del agente.\n",
        "* Luego, use esta funci칩n para entregar la acci칩n escogida frente a los siguientes escenarios:\n",
        "\n",
        "  * Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
        "  * Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
        "\n",
        "* 쯉on coherentes sus acciones con las reglas del juego?\n",
        "\n",
        "Hint: 쮸 que clase de python pertenecen los estados? Pruebe a usar el m칠todo `.reset` para saberlo."
      ],
      "metadata": {
        "id": "RO-EsAaPAYEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# escriba su respuesta ac치\n",
        "# Funci칩n para predecir la acci칩n\n",
        "def predict_action(state):\n",
        "    state = np.array(state).flatten().reshape(1, -1)\n",
        "    action, _ = model.predict(state, deterministic=True)\n",
        "    return action\n",
        "\n",
        "# Escenarios dados\n",
        "state1 = (6, 7, 0)  # Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene un as\n",
        "state2 = (19, 3, 1)  # Suma de cartas del agente es 19, dealer muestra un 3, agente tiene un as\n",
        "\n",
        "# Obtener las acciones\n",
        "action1 = predict_action(state1)\n",
        "action2 = predict_action(state2)\n",
        "\n",
        "print(f\"Acci칩n para el estado 1: {action1}\")\n",
        "print(f\"Acci칩n para el estado 2: {action2}\")"
      ],
      "metadata": {
        "id": "Lssdp7AvGaRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008845f1-dd85-421b-9967-ef593e162153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acci칩n para el estado 1: [1]\n",
            "Acci칩n para el estado 2: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "묖tEstado 1: Acci칩n [1] corresponde a 랋edir carta. Con una suma de 6 y el crupier mostrando un 7, es coherente que el agente pida carta, ya que la suma es muy baja.\n",
        "\n",
        "묖tEstado 2: Acci칩n [0] corresponde a 랋lantarse. Con una suma de 19 y el crupier mostrando un 3, es coherente plantarse, ya que la suma es alta y el riesgo de pasarse es considerable.\n",
        "\n",
        "Ambas acciones son coherentes con las estrategias t칤picas en Blackjack."
      ],
      "metadata": {
        "id": "gRXVON2WG0UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 LunarLander**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la secci칩n 2.1, en esta secci칩n usted se encargar치 de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n"
      ],
      "metadata": {
        "id": "SEqCTqqroh03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.1 Descripci칩n de MDP (0.2 puntos)**\n"
      ],
      "metadata": {
        "id": "sk5VJVppXh3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comencemos preparando el ambiente:"
      ],
      "metadata": {
        "id": "XvAVq1wQIjLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\", continuous = True) # notar el par치metro continuous = True"
      ],
      "metadata": {
        "id": "Qb5PmadJIngR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Entregue una breve descripci칩n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas.\n",
        "* 쮺omo se distinguen las acciones de este ambiente en comparaci칩n a `Blackjack`?\n",
        "* En la preparaci칩n del ambiente se especifica el par치metro `continuous = True`. 쯈ue implicancias tiene esto sobre el ambiente?"
      ],
      "metadata": {
        "id": "LNERH-m8JYQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descripci칩n del ambiente Lunar Lander:\n",
        "\n",
        "묖tEstados: Es un vector de 8 dimensiones que incluye coordenadas, velocidades, 치ngulo, velocidad angular y contacto de las patas con el suelo.\n",
        "묖tAcciones: Cuatro acciones discretas (sin motor, motor principal, motor lateral izquierdo y derecho).\n",
        "묖tRecompensas: Basadas en la proximidad al objetivo, velocidad, inclinaci칩n, contacto de las patas y el uso de motores.\n",
        "\n",
        "Comparaci칩n con Blackjack:\n",
        "\n",
        "En Lunar Lander, las acciones son m치s complejas, involucrando control de motores y orientaci칩n, mientras que en Blackjack solo son decisiones de 랋edir o 랋lantarse.\n",
        "\n",
        "Implicancias de continuous = True:\n",
        "\n",
        "Cuando continuous = True, las acciones se vuelven continuas, permitiendo un control m치s preciso del motor principal y los motores laterales, en lugar de simplemente encender o apagar motores."
      ],
      "metadata": {
        "id": "DbpGahPcHAje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "* Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 10 veces y reporte el promedio y desviaci칩n de las recompensas.\n",
        "* 쮺칩mo calificar칤a el performance de esta pol칤tica?"
      ],
      "metadata": {
        "id": "YChodtNQwzG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Crear el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\", continuous=True)\n",
        "\n",
        "# Simular 10 episodios con acciones aleatorias\n",
        "n_episodes = 10\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    obs, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Acci칩n aleatoria\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviaci칩n est치ndar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "id": "pNMT_GORIreW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e41e3ef-6a08-4597-d8f4-a716ee2e3f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -246.5308163586496\n",
            "Desviaci칩n est치ndar de las recompensas: 211.09166153352746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El performance de la pol칤tica aleatoria es bajo, con un promedio de recompensas de aproximadamente -246.53. Esto indica que, en general, el agente tiene dificultades para aterrizar correctamente. La alta desviaci칩n est치ndar de 211.09 sugiere una gran variabilidad en los resultados, lo que es t칤pico de una pol칤tica que no toma decisiones informadas."
      ],
      "metadata": {
        "id": "d1n-P5XHH6DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "* A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
      ],
      "metadata": {
        "id": "hQrZVQflX_5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Crear el modelo DDPG\n",
        "model = DDPG(\"MlpPolicy\", env, verbose=1, seed=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"ddpg_lunar_lander\")\n",
        "\n",
        "# Cargar el modelo\n",
        "model = DDPG.load(\"ddpg_lunar_lander\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Mg0epSnLKfy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7fdd28-4a0e-41a2-a5d2-d68042bb8fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 7        |\n",
            "|    total_timesteps | 381      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 10.7     |\n",
            "|    critic_loss     | 21.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 280      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 38       |\n",
            "|    time_elapsed    | 17       |\n",
            "|    total_timesteps | 683      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 23.4     |\n",
            "|    critic_loss     | 22.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 582      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 62       |\n",
            "|    total_timesteps | 1967     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 22.2     |\n",
            "|    critic_loss     | 25       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1866     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 30       |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 2625     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.1     |\n",
            "|    critic_loss     | 39.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 2524     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 30       |\n",
            "|    time_elapsed    | 112      |\n",
            "|    total_timesteps | 3472     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 14.1     |\n",
            "|    critic_loss     | 39.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3371     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 145      |\n",
            "|    total_timesteps | 4557     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.19     |\n",
            "|    critic_loss     | 9.88     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 4456     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 30       |\n",
            "|    time_elapsed    | 195      |\n",
            "|    total_timesteps | 6029     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -3.22    |\n",
            "|    critic_loss     | 29.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 5928     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 255      |\n",
            "|    total_timesteps | 7961     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.7    |\n",
            "|    critic_loss     | 20.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7860     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 270      |\n",
            "|    total_timesteps | 8434     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -30.1    |\n",
            "|    critic_loss     | 26.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8333     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 31       |\n",
            "|    time_elapsed    | 298      |\n",
            "|    total_timesteps | 9347     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.1    |\n",
            "|    critic_loss     | 19.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9246     |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
        "\n",
        "* Repita el ejercicio 2.2.2 pero utilizando el modelo entrenado.\n",
        "* 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
      ],
      "metadata": {
        "id": "3z-oIUSrlAsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = DDPG.load(\"ddpg_lunar_lander\")\n",
        "\n",
        "# Simular 10 episodios con el modelo entrenado\n",
        "n_episodes = 10\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward[0]\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviaci칩n est치ndar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "id": "CWVY1a39KeRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8f40c8-1d45-4d46-d424-fe22247f77ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -76.05608874436003\n",
            "Desviaci칩n est치ndar de las recompensas: 48.65351366634402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance del Agente:\n",
        "El agente entrenado tiene un promedio de recompensas de -76.06 con una desviaci칩n est치ndar de 48.65. Esto muestra una mejora significativa respecto al baseline.\n",
        "\n",
        "Comparaci칩n con el Escenario Baseline:\n",
        "Comparado con el baseline, donde el promedio de recompensas era -246.53, el agente es claramente mejor. Las recompensas m치s altas y la menor desviaci칩n est치ndar indican un comportamiento m치s consistente y efectivo."
      ],
      "metadata": {
        "id": "RZe251MWUyrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.5 Optimizaci칩n de modelo (0.2 puntos)**\n",
        "\n",
        "* Repita los ejercicios 2.2.3 y 2.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par치metros como:\n",
        "  - `total_timesteps`\n",
        "  - `learning_rate`\n",
        "  - `batch_size`\n",
        "\n",
        "* Una vez optimizado el modelo, use la funci칩n `export_gif` entregada para estudiar el comportamiento de su agente en la resoluci칩n del ambiente, comente sobre sus resultados.\n",
        "\n",
        "* Adjunte el gif generado en su entrega. Si, adem치s, adjuntan el gif en el markdown tendr치n un bonus de 0.1."
      ],
      "metadata": {
        "id": "x6Xw4YHT3P5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def export_gif(model, env, n=5):\n",
        "    '''\n",
        "    Funci칩n que exporta a gif el comportamiento del agente en n episodios.\n",
        "    '''\n",
        "    images = []\n",
        "    for episode in range(n):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            img = env.render(mode=\"rgb_array\")\n",
        "            if img is not None:\n",
        "                images.append(img)\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    if images:\n",
        "        imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i % 2 == 0], fps=29)\n"
      ],
      "metadata": {
        "id": "pwL5djUkMS3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creando un modelo que cambie los parametros.\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Ajustar par치metros del modelo\n",
        "model = DDPG(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    verbose=1,\n",
        "    seed=42,\n",
        "    learning_rate=0.0001,  # Ajusta el learning rate\n",
        "    batch_size=64         # Ajusta el batch size\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=20000)  # Aumenta el total_timesteps\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"ddpg_lunar_lander_optimized\")\n",
        "\n",
        "# Cargar el modelo\n",
        "model = DDPG.load(\"ddpg_lunar_lander_optimized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dv0q01RNXls",
        "outputId": "8d09cb28-1920-47b2-8bea-206350326f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 73       |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 354      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 5.97     |\n",
            "|    critic_loss     | 105      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 253      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 70       |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 669      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.45     |\n",
            "|    critic_loss     | 124      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 568      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 63       |\n",
            "|    time_elapsed    | 19       |\n",
            "|    total_timesteps | 1216     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 1.76     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1115     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 61       |\n",
            "|    time_elapsed    | 26       |\n",
            "|    total_timesteps | 1628     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 9.95     |\n",
            "|    critic_loss     | 72.8     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1527     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 2231     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.6     |\n",
            "|    critic_loss     | 6.31     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2130     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 44       |\n",
            "|    total_timesteps | 2690     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.8     |\n",
            "|    critic_loss     | 37.8     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2589     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 58       |\n",
            "|    total_timesteps | 3538     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 10.5     |\n",
            "|    critic_loss     | 1.2      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 3437     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 68       |\n",
            "|    total_timesteps | 4156     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 13.8     |\n",
            "|    critic_loss     | 3.34     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 4055     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 79       |\n",
            "|    total_timesteps | 4808     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 20.4     |\n",
            "|    critic_loss     | 47.4     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 4707     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 61       |\n",
            "|    time_elapsed    | 95       |\n",
            "|    total_timesteps | 5809     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.3     |\n",
            "|    critic_loss     | 9.04     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 5708     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 101      |\n",
            "|    total_timesteps | 6190     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 14.9     |\n",
            "|    critic_loss     | 41.7     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 6089     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 111      |\n",
            "|    total_timesteps | 6790     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 19.2     |\n",
            "|    critic_loss     | 9.21     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 6689     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 123      |\n",
            "|    total_timesteps | 7536     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 12       |\n",
            "|    critic_loss     | 65.1     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 7435     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 134      |\n",
            "|    total_timesteps | 8121     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 32       |\n",
            "|    critic_loss     | 14.2     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 8020     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 143      |\n",
            "|    total_timesteps | 8626     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 22       |\n",
            "|    critic_loss     | 9.74     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 8525     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 167      |\n",
            "|    total_timesteps | 10114    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18       |\n",
            "|    critic_loss     | 5.63     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 10013    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 177      |\n",
            "|    total_timesteps | 10785    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 22.8     |\n",
            "|    critic_loss     | 7.87     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 10684    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 193      |\n",
            "|    total_timesteps | 11734    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 25.7     |\n",
            "|    critic_loss     | 5.5      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 11633    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 212      |\n",
            "|    total_timesteps | 12866    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 10.4     |\n",
            "|    critic_loss     | 1.68     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 12765    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 224      |\n",
            "|    total_timesteps | 13566    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.43     |\n",
            "|    critic_loss     | 0.922    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 13465    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 238      |\n",
            "|    total_timesteps | 14313    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.4     |\n",
            "|    critic_loss     | 19.3     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 14212    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 259      |\n",
            "|    total_timesteps | 15650    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.4     |\n",
            "|    critic_loss     | 9.38     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 15549    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 272      |\n",
            "|    total_timesteps | 16445    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 13.7     |\n",
            "|    critic_loss     | 24.4     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 16344    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 287      |\n",
            "|    total_timesteps | 17363    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 16.5     |\n",
            "|    critic_loss     | 21.2     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 17262    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 311      |\n",
            "|    total_timesteps | 18784    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.47     |\n",
            "|    critic_loss     | 1.8      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 18683    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 60       |\n",
            "|    time_elapsed    | 328      |\n",
            "|    total_timesteps | 19844    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.43     |\n",
            "|    critic_loss     | 2.49     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 19743    |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y envolver el ambiente\n",
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Cargar el modelo\n",
        "model = DDPG.load(\"ddpg_lunar_lander_optimized\")\n",
        "\n",
        "# Simular 10 episodios con el modelo entrenado\n",
        "n_episodes = 10\n",
        "rewards = []\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward[0]\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar\n",
        "mean_reward = np.mean(rewards)\n",
        "std_reward = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de recompensas: {mean_reward}\")\n",
        "print(f\"Desviaci칩n est치ndar de las recompensas: {std_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugtbmpp9NaBH",
        "outputId": "1272936f-41e1-4d33-c879-7f6de27315d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -58.281686818308664\n",
            "Desviaci칩n est치ndar de las recompensas: 64.18256240944172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#usando la funci칩n\n",
        "export_gif(model, env, n=5)"
      ],
      "metadata": {
        "id": "VvSTimK0SIuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BALqZZGfUkx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}